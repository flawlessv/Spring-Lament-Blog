# RAG 流式输出实现文档

## 功能概述

实现 RAG 智能问答的流式输出（打字机效果），让 AI 的回答逐字显示，提升用户体验。

## 核心问题解答

**Q: 请求是发了一次还是多次？**
A: **只发一次请求**。前端发送一个 POST 请求到 `/api/ai/rag/stream`，然后保持连接打开，持续接收数据。

**Q: 响应是响应了一次还是多次？**
A: **响应是一次，但数据是分多次推送的**。服务器通过 SSE（Server-Sent Events）协议，在同一个 HTTP 响应中分多次推送数据块，直到回答生成完成。

## 整体流程

整个流程是这样的：

1. **用户提问** → 前端发送一个 POST 请求
2. **后端接收请求** → 创建 SSE 流，保持连接打开
3. **向量检索** → 同步完成，立即发送来源信息（sources 事件）
4. **调用 Kimi API** → 使用流式模式，Kimi 逐块返回文本
5. **后端转发** → 每收到一个文本块，就通过 SSE 发送给前端（chunk 事件）
6. **前端接收** → 实时解析 SSE 数据，更新 UI，实现打字机效果
7. **回答完成** → 发送完成事件（complete 事件），关闭连接

**关键点**：

- 整个过程中只有**一个 HTTP 请求**，但数据是**分多次推送**的
- 使用 SSE 协议，服务器可以主动推送数据，不需要前端轮询
- 前端使用 `ReadableStream` 读取流式数据，实时更新 UI

## 技术方案

### 核心技术

1. **Server-Sent Events (SSE)** - 服务器主动推送数据，保持长连接
2. **ReadableStream** - 浏览器流式读取响应，实时处理数据
3. **OpenAI SDK 流式 API** - Kimi API 支持流式输出，逐块返回文本

---

## 实现细节

### 1. AI 客户端流式调用

**文件**: `src/lib/ai/client.ts`

核心是调用 Kimi API 时设置 `stream: true`，这样 API 会逐块返回文本，而不是等全部生成完再返回。

关键代码：

```typescript
// 创建流式请求
const stream = await this.client.chat.completions.create({
  stream: true, // 启用流式输出
  // ... 其他参数
});

// 逐块处理流式响应
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || "";
  if (content) {
    onChunk?.(content); // 实时回调每个文本块
  }
}
```

**工作原理**：

- `stream: true` 告诉 Kimi API 使用流式模式
- `for await...of` 循环会等待每个数据块，Kimi 每生成一点文本就返回一个块
- `onChunk` 回调函数在每次收到文本块时被调用，可以实时传递给前端

---

### 2. RAG 流式查询

**文件**: `src/lib/ai/rag.ts`

#### 回调接口设计

```typescript
export interface RAGStreamCallbacks {
  onSources?: (sources: RAGResponse["sources"]) => void; // 来源信息
  onChunk?: (chunk: string) => void; // 文本块
  onComplete?: (result: {
    tokensUsed?: number;
    mode?: "rag" | "fallback";
  }) => void; // 完成
  onError?: (error: Error) => void; // 错误
}
```

#### 流式查询流程

```typescript
export async function ragQueryStream(
  question: string,
  options: RAGOptions = {},
  callbacks: RAGStreamCallbacks = {}
): Promise<void> {
  // 1. 向量检索（非流式）
  const searchResults = await vectorStore.search(queryVector, { limit: 5 });

  // 2. 发送来源信息（立即）
  callbacks.onSources?.(sources);

  // 3. 构建上下文和 Prompt
  const prompt = buildRAGPrompt(question, context);

  // 4. 流式调用 LLM
  const result = await aiClient.chatStream(
    messages,
    options,
    callbacks.onChunk // 每个文本块都会触发回调
  );

  // 5. 完成回调
  callbacks.onComplete?.({
    mode: "rag",
    tokensUsed: result.tokensUsed,
  });
}
```

**设计思路**：

- 向量检索是同步的，先完成再开始流式输出
- 来源信息立即发送，用户可以提前看到参考文章
- LLM 回答部分使用流式输出，实现打字机效果

---

### 3. 流式 API 路由

**文件**: `src/app/api/ai/rag/stream/route.ts`

这是整个流程的核心。API 路由创建一个 `ReadableStream`，保持连接打开，然后通过 SSE 协议推送数据。

**SSE 数据格式**：

```
event: sources
data: {"sources": [...]}

event: chunk
data: {"chunk": "文本块"}

event: complete
data: {"tokensUsed": 150, "mode": "rag"}

event: error
data: {"error": "错误信息"}
```

**工作原理**：

1. 创建 `ReadableStream`，保持连接打开
2. 执行 RAG 查询，通过回调函数接收数据
3. 每次收到数据（来源、文本块、完成、错误），就按照 SSE 格式推送给前端
4. 响应头设置为 `text/event-stream`，告诉浏览器这是流式数据

关键点：

- `ReadableStream` 允许服务器持续推送数据
- SSE 格式：`event: 事件类型\ndata: JSON数据\n\n`
- 响应头 `Connection: keep-alive` 保持连接打开

---

### 4. 前端流式读取

**文件**: `src/components/admin/rag-chat.tsx`

前端使用 `fetch` API 发起请求，然后通过 `ReadableStream` 读取流式数据。

**工作流程**：

1. 发送 POST 请求，获取响应对象
2. 使用 `response.body.getReader()` 获取流式读取器
3. 循环读取数据块，使用 `TextDecoder` 解码
4. 解析 SSE 格式，提取事件类型和数据
5. 根据事件类型更新 React 状态：
   - `sources` 事件：更新参考来源
   - `chunk` 事件：追加文本内容，实现打字机效果
   - `complete` 事件：标记完成
   - `error` 事件：显示错误

**关键点**：

- 使用 `buffer` 处理不完整的 SSE 消息（可能一次读取到多个事件，或一个事件分多次读取）
- 每次收到 `chunk` 事件就更新状态，触发 UI 重渲染，实现打字机效果
- 状态更新使用函数式更新，确保拿到最新状态

---

## 完整数据流

```
1. 用户输入问题
   ↓
2. 前端发送一个 POST 请求到 /api/ai/rag/stream
   ↓
3. 后端创建 ReadableStream，保持连接打开
   ↓
4. 执行向量检索（同步，很快完成）
   ↓
5. 立即发送 sources 事件 → 前端显示参考来源
   ↓
6. 构建 Prompt，调用 aiClient.chatStream
   ↓
7. Kimi API 开始流式返回文本
   ↓
8. 每收到一个文本块 → 触发 onChunk 回调
   ↓
9. 后端通过 SSE 发送 chunk 事件
   ↓
10. 前端 ReadableStream 读取数据
   ↓
11. 解析 SSE 格式，更新 React 状态
   ↓
12. UI 实时显示（打字机效果）
   ↓
13. 回答生成完成 → 发送 complete 事件 → 关闭连接
```

**关键理解**：

- 整个过程只有**一个 HTTP 请求**，但数据是**持续推送**的
- 向量检索完成后立即发送来源，用户不用等 AI 生成完就能看到参考文章
- AI 生成是流式的，每生成一点就推送一点，实现打字机效果

---

## 优势与特点

### 1. 用户体验提升

- **即时反馈**：用户立即看到 AI 开始回答
- **打字机效果**：逐字显示，更有交互感
- **来源提前显示**：在回答生成前就能看到参考文章

### 2. 性能优化

- **首字节时间 (TTFB) 降低**：不需要等待完整回答
- **感知延迟降低**：用户感觉响应更快
- **可中断性**：理论上可以中断流式请求（未实现）

### 3. 技术优势

- **标准协议**：使用 SSE 标准，兼容性好
- **自动重连**：EventSource 支持自动重连（当前使用 fetch，可切换）
- **错误处理**：流式错误也能及时传递

---

## 注意事项

### 1. 错误处理

- 流式过程中可能出现网络中断
- 需要处理不完整的 JSON 数据
- 前端需要处理流式错误事件

### 2. 性能考虑

- 频繁的状态更新可能影响性能（当前实现已优化）
- 长回答会产生大量小更新（可考虑批量更新）

### 3. 兼容性

- SSE 需要 HTTP/1.1 或 HTTP/2
- 某些代理服务器可能不支持流式传输
- 移动网络可能不稳定

---

## 后续优化方向

1. **批量更新**：累积多个 chunk 后批量更新，减少渲染次数
2. **中断功能**：允许用户中断流式请求
3. **重连机制**：网络中断时自动重连
4. **进度指示**：显示回答生成进度
5. **降级方案**：流式失败时自动降级为普通请求

---

## 总结

流式输出的核心是：**一个请求，多次推送**。

技术实现：

1. **后端**：使用 `ReadableStream` 创建 SSE 流，保持连接打开，逐块推送数据
2. **AI 调用**：使用 OpenAI SDK 的 `stream: true`，Kimi API 逐块返回文本
3. **前端**：使用 `fetch` + `ReadableStream` 读取 SSE，实时解析并更新 UI

这样既保证了用户体验（打字机效果、即时反馈），又保持了代码的简洁性和可维护性。
