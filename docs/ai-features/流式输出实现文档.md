# RAG 流式输出实现文档

## 功能概述

实现 RAG 智能问答的流式输出（打字机效果），让 AI 的回答逐字显示，提升用户体验。

## 技术方案

### 架构设计

TODO:这个文章没解释清楚，这里发请求是发了一次还是多次？响应请求是响应了一次还是多次？
还有下面的流程图也看不懂啥玩意，贴太多代码但是并没有解释

```
前端组件 (rag-chat.tsx)
    ↓ fetch + ReadableStream
流式 API (/api/ai/rag/stream)
    ↓ Server-Sent Events (SSE)
RAG 流式查询 (ragQueryStream)
    ↓ 流式 AI 调用
Kimi API (chatStream)
    ↓ 逐块返回
前端实时更新 UI
```

### 核心技术

1. **Server-Sent Events (SSE)** - 服务器主动推送数据
2. **ReadableStream** - 浏览器流式读取响应
3. **OpenAI SDK 流式 API** - Kimi API 支持流式输出

---

## 实现细节

### 1. AI 客户端扩展

**文件**: `src/lib/ai/client.ts`

#### 接口扩展

```typescript
export interface AIClient {
  chat(messages: ChatMessage[], options?: ChatOptions): Promise<ChatResponse>;
  chatStream(
    messages: ChatMessage[],
    options?: ChatOptions,
    onChunk?: (chunk: string) => void
  ): Promise<ChatResponse>;
  embed(text: string | string[]): Promise<number[][]>;
}
```

#### 流式实现

```typescript
async chatStream(
  messages: ChatMessage[],
  options: ChatOptions = {},
  onChunk?: (chunk: string) => void
): Promise<ChatResponse> {
  // 1. 创建流式请求
  const stream = await this.client.chat.completions.create({
    model: options.model || process.env.KIMI_MODEL || "moonshot-v1-32k",
    messages: messages.map((m) => ({
      role: m.role,
      content: m.content,
    })),
    temperature: options.temperature ?? 0.7,
    max_tokens: options.maxTokens ?? 2000,
    stream: true, // 关键：启用流式输出
  });

  let fullContent = "";
  let tokensUsed = 0;

  // 2. 逐块处理流式响应
  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content || "";
    if (content) {
      fullContent += content;
      onChunk?.(content); // 实时回调每个文本块
    }

    if (chunk.usage?.total_tokens) {
      tokensUsed = chunk.usage.total_tokens;
    }
  }

  return {
    content: fullContent,
    tokensUsed: tokensUsed || undefined,
  };
}
```

**关键点**：

- `stream: true` 启用 OpenAI SDK 的流式模式
- `for await...of` 异步迭代器处理流式数据
- `onChunk` 回调函数实时传递文本块

---

### 2. RAG 流式查询

**文件**: `src/lib/ai/rag.ts`

#### 回调接口设计

```typescript
export interface RAGStreamCallbacks {
  onSources?: (sources: RAGResponse["sources"]) => void; // 来源信息
  onChunk?: (chunk: string) => void; // 文本块
  onComplete?: (result: {
    tokensUsed?: number;
    mode?: "rag" | "fallback";
  }) => void; // 完成
  onError?: (error: Error) => void; // 错误
}
```

#### 流式查询流程

```typescript
export async function ragQueryStream(
  question: string,
  options: RAGOptions = {},
  callbacks: RAGStreamCallbacks = {}
): Promise<void> {
  // 1. 向量检索（非流式）
  const searchResults = await vectorStore.search(queryVector, { limit: 5 });

  // 2. 发送来源信息（立即）
  callbacks.onSources?.(sources);

  // 3. 构建上下文和 Prompt
  const prompt = buildRAGPrompt(question, context);

  // 4. 流式调用 LLM
  const result = await aiClient.chatStream(
    messages,
    options,
    callbacks.onChunk // 每个文本块都会触发回调
  );

  // 5. 完成回调
  callbacks.onComplete?.({
    mode: "rag",
    tokensUsed: result.tokensUsed,
  });
}
```

**设计思路**：

- 向量检索是同步的，先完成再开始流式输出
- 来源信息立即发送，用户可以提前看到参考文章
- LLM 回答部分使用流式输出，实现打字机效果

---

### 3. 流式 API 路由

**文件**: `src/app/api/ai/rag/stream/route.ts`

#### SSE 格式

Server-Sent Events 使用特定格式：

```
event: sources
data: {"sources": [...]}

event: chunk
data: {"chunk": "文本块"}

event: complete
data: {"tokensUsed": 150, "mode": "rag"}

event: error
data: {"error": "错误信息"}
```

#### 实现代码

```typescript
export async function POST(request: NextRequest) {
  // 1. 创建 ReadableStream
  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      // 2. 发送事件的辅助函数
      const sendEvent = (type: string, data: any) => {
        const message = `event: ${type}\ndata: ${JSON.stringify(data)}\n\n`;
        controller.enqueue(encoder.encode(message));
      };

      // 3. 执行流式 RAG 查询
      await ragQueryStream(question, options, {
        onSources: (sources) => {
          sendEvent("sources", { sources });
        },
        onChunk: (chunk) => {
          sendEvent("chunk", { chunk });
        },
        onComplete: (result) => {
          sendEvent("complete", result);
          controller.close(); // 关闭流
        },
        onError: (error) => {
          sendEvent("error", { error: error.message });
          controller.close();
        },
      });
    },
  });

  // 4. 返回流式响应
  return new Response(stream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
```

**关键点**：

- `ReadableStream` 用于创建可读流
- `TextEncoder` 将字符串编码为字节
- SSE 格式：`event: type\ndata: json\n\n`
- 响应头必须包含 `text/event-stream`

---

### 4. 前端流式读取

**文件**: `src/components/admin/rag-chat.tsx`

#### 使用 Fetch API 读取流

```typescript
const handleSend = async () => {
  // 1. 创建消息占位符
  const assistantMessage: Message = {
    id: `msg_${Date.now()}`,
    role: "assistant",
    content: "", // 初始为空，逐步填充
    timestamp: new Date(),
  };
  setMessages((prev) => [...prev, assistantMessage]);

  // 2. 发起流式请求
  const response = await fetch("/api/ai/rag/stream", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ question: userMessage.content }),
  });

  // 3. 获取流式读取器
  const reader = response.body?.getReader();
  const decoder = new TextDecoder();

  let buffer = "";

  // 4. 逐块读取
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });

    // 5. 解析 SSE 格式
    const chunks = buffer.split("\n\n");
    buffer = chunks.pop() || "";

    for (const chunk of chunks) {
      let eventType = "";
      let dataStr = "";

      for (const line of chunk.split("\n")) {
        if (line.startsWith("event: ")) {
          eventType = line.slice(7).trim();
        } else if (line.startsWith("data: ")) {
          dataStr = line.slice(6);
        }
      }

      if (dataStr) {
        const data = JSON.parse(dataStr);

        // 6. 根据事件类型更新 UI
        if (eventType === "sources") {
          setMessages((prev) =>
            prev.map((msg) =>
              msg.id === assistantMessageId
                ? { ...msg, sources: data.sources }
                : msg
            )
          );
        }

        if (eventType === "chunk") {
          setMessages((prev) =>
            prev.map((msg) =>
              msg.id === assistantMessageId
                ? { ...msg, content: msg.content + data.chunk }
                : msg
            )
          );
        }
      }
    }
  }
};
```

**关键点**：

- `response.body.getReader()` 获取流式读取器
- `TextDecoder` 将字节解码为字符串
- 需要处理不完整的 SSE 消息（使用 buffer）
- 实时更新 React 状态，触发 UI 重渲染

---

## 数据流图

```
用户输入问题
    ↓
前端发送 POST /api/ai/rag/stream
    ↓
后端创建 ReadableStream
    ↓
执行向量检索 → 发送 sources 事件
    ↓
构建 Prompt → 调用 aiClient.chatStream
    ↓
Kimi API 流式返回 → 逐块触发 onChunk
    ↓
后端发送 chunk 事件 (SSE)
    ↓
前端 ReadableStream 读取
    ↓
解析 SSE 格式 → 更新 React 状态
    ↓
UI 实时显示（打字机效果）
    ↓
流结束 → 发送 complete 事件
```

---

## 优势与特点

### 1. 用户体验提升

- **即时反馈**：用户立即看到 AI 开始回答
- **打字机效果**：逐字显示，更有交互感
- **来源提前显示**：在回答生成前就能看到参考文章

### 2. 性能优化

- **首字节时间 (TTFB) 降低**：不需要等待完整回答
- **感知延迟降低**：用户感觉响应更快
- **可中断性**：理论上可以中断流式请求（未实现）

### 3. 技术优势

- **标准协议**：使用 SSE 标准，兼容性好
- **自动重连**：EventSource 支持自动重连（当前使用 fetch，可切换）
- **错误处理**：流式错误也能及时传递

---

## 注意事项

### 1. 错误处理

- 流式过程中可能出现网络中断
- 需要处理不完整的 JSON 数据
- 前端需要处理流式错误事件

### 2. 性能考虑

- 频繁的状态更新可能影响性能（当前实现已优化）
- 长回答会产生大量小更新（可考虑批量更新）

### 3. 兼容性

- SSE 需要 HTTP/1.1 或 HTTP/2
- 某些代理服务器可能不支持流式传输
- 移动网络可能不稳定

---

## 后续优化方向

1. **批量更新**：累积多个 chunk 后批量更新，减少渲染次数
2. **中断功能**：允许用户中断流式请求
3. **重连机制**：网络中断时自动重连
4. **进度指示**：显示回答生成进度
5. **降级方案**：流式失败时自动降级为普通请求

---

## 总结

流式输出通过 **SSE + ReadableStream + 流式 AI API** 实现，核心是：

1. **后端**：使用 `ReadableStream` 创建 SSE 流，逐块发送数据
2. **AI 调用**：使用 OpenAI SDK 的 `stream: true` 选项
3. **前端**：使用 `fetch` + `ReadableStream` 读取 SSE，实时更新 UI

这种方案既保证了用户体验，又保持了代码的简洁性和可维护性。
