# 博客系统 AI 功能亮点逐字稿

## 第一部分：AI Tab 补全 - 类似 Copilot 的智能写作助手

### 问题背景

在开发这个博客系统时,我希望为写作提供更智能的辅助体验。传统的编辑器要么只有基础的文本编辑功能,要么需要手动打开侧边栏、输入需求、等待 AI 生成,然后再复制回来,这个流程非常繁琐,严重打断写作思路。

我想要的是一种无缝的体验,就像 GitHub Copilot 在写代码时那样——当你在写文章的时候,AI 能自动感知你的写作意图,在光标位置实时显示补全建议,按 Tab 键就能接受,完全不打断写作流程。但这个需求有几个核心挑战:

**第一个挑战是如何实现非侵入式的视觉提示**。补全建议需要直接显示在光标位置,以灰色斜体文本的形式呈现,但又不能影响文档的实际内容。如果直接插入到文档中,用户继续输入时会把建议文本也带上,这显然不对。而且建议文本必须能随时清除,不能留下任何痕迹。

**第二个挑战是性能优化**。用户在快速输入时,每次按键都会触发状态变化。如果每次输入都立即调用 AI API,会导致大量无效请求,既浪费资源又影响性能。而且 AI 生成需要时间,用户可能在 API 返回前就继续输入了,这时如果显示过期的建议,反而会干扰用户。

**第三个挑战是异步竞态条件**。AI API 调用是异步的,返回时间不确定。假设用户在第 10 个字符位置输入后触发了 API 调用,然后继续输入到第 20 个字符位置,这时第一个 API 调用才返回。如果直接显示这个建议,它会出现在错误的位置上,导致用户体验很差。

### 核心思路

我的解决思路是基于 Tiptap(底层是 ProseMirror)的插件机制,设计了一个**状态驱动 + 防抖优化 + 双重位置校验**的架构。

整体架构分为三层:

**第一层是状态管理层**。我使用 ProseMirror Plugin 的 state 机制来管理补全状态,状态只包含两个字段:suggestion(补全文本)和 position(补全位置)。所有对补全状态的修改都通过事务的元数据(meta)来进行,这确保了状态变化的可追溯性和一致性。

**第二层是渲染层**。我使用 Decoration.widget 来渲染补全建议。widget 是 ProseMirror 提供的一种机制,它可以在文档中插入 DOM 元素,但这个元素是纯视觉的,不会影响文档结构。补全建议显示为灰色斜体文本,且设置了 pointer-events: none 和 user-select: none,确保它不会拦截鼠标事件或被选中,完全不影响用户的编辑操作。

**第三层是防抖和校验层**。我实现了 500ms 的防抖机制,当用户快速输入时,会不断清除之前的定时器,只保留最后一次输入的定时器。而且我设计了双重位置校验:第一次校验在防抖回调执行时,检查光标位置是否改变;第二次校验在 API 返回后,再次检查位置。只有两次校验都通过,补全建议才会显示。

具体的工作流程是这样的:

1. 用户在编辑器中输入文本,触发 ProseMirror 的状态变化
2. 插件的 apply 方法被调用,检测到文本变化
3. 进行一系列边界检查:是否选择了文本、是否在代码块中、文本是否太短
4. 通过边界检查后,清除之前的防抖定时器,设置新的 500ms 定时器
5. 500ms 后,定时器触发,进行第一次位置校验,如果位置已改变则放弃
6. 提取光标前的内容作为上下文,调用 AI API
7. API 返回后,进行第二次位置校验,如果位置仍然一致,则更新插件状态
8. 状态更新触发 decorations 方法,渲染补全建议
9. 用户按 Tab 键接受补全,或按 Esc 键取消

### 技术难点与解决方案

#### 难点一:异步竞态条件的彻底解决

最开始我只做了一次位置校验,在 API 返回后检查位置。但测试时发现,在网络较慢的情况下,用户可能在防抖延迟期间就继续输入了,这时 API 调用已经发出,但上下文已经过期。

我的解决方案是实现双重校验:

**第一次校验**在防抖回调执行时进行。此时距离用户最后一次输入已经过了 500ms,我重新获取编辑器状态,检查光标位置是否仍然在预期位置。如果用户在这 500ms 内继续输入了,位置会不一致,这时直接放弃 API 调用,避免浪费资源。

**第二次校验**在 API 返回后进行。因为 API 调用也需要时间,用户可能在等待期间继续输入。所以在收到 API 响应后,我再次获取最新的编辑器状态,检查位置是否仍然一致。只有两次校验都通过,补全建议才会显示。

这种双重校验机制确保了补全建议永远只显示在正确的位置上,即使在快速输入或网络延迟的情况下也不会出错。

#### 难点二:防抖定时器的正确管理

防抖机制看似简单,但在 ProseMirror 插件中实现起来有坑。最大的问题是定时器的作用域管理。

如果把定时器声明在 apply 方法内部,每次 apply 执行时都会创建一个新的局部变量,无法清除之前的定时器,导致多个定时器并行执行,失去了防抖的意义。

我的解决方案是将定时器声明在插件闭包的外层,在模块级别创建一个 debounceTimer 变量。这样所有 apply 调用都能访问到同一个定时器引用,每次新输入时都能正确清除之前的定时器。

```typescript
// 在插件闭包外层声明,所有 apply 调用共享
let debounceTimer: NodeJS.Timeout | null = null;

// 在 apply 方法中
if (debounceTimer) {
  clearTimeout(debounceTimer); // 清除之前的定时器
  debounceTimer = null;
}

debounceTimer = setTimeout(async () => {
  // API 调用逻辑
}, 500);
```

#### 难点三:Decoration 的正确使用

Decoration 是 ProseMirror 的核心机制,但文档不够详细,容易用错。我遇到的第一个问题是 side 参数的理解。

Decoration.widget 的 side 参数决定了装饰在位置的左边还是右边。如果设置为 -1(左边),补全建议会显示在光标左边,看起来像是在已输入文本的后面,这是错误的。设置为 1(右边),补全建议才会显示在光标右边,也就是待输入位置,这才是正确的效果。

第二个问题是装饰的样式设置。最开始我没有设置 pointer-events: none,结果用户点击补全建议时,鼠标事件被装饰元素拦截了,光标位置发生了改变。后来添加了这个属性,确保装饰元素完全透明,不拦截任何交互。

第三个问题是装饰的位置校验。在 decorations 方法中,我需要再次校验位置的有效性。因为文档可能在状态更新和渲染之间发生了变化,如果位置超出文档范围,调用 state.doc.resolve 会抛出异常。我用 try-catch 包裹了这个调用,并在 catch 块中返回空的装饰集合,确保不会因为异常导致编辑器崩溃。

#### 难点四:边界情况的全面处理

在实际使用中,我发现了很多边界情况需要处理:

**情况一:用户选择了文本**。如果用户选择了一段文本,可能是想要删除或替换,这时显示补全建议是不合适的。我通过检查 selection.from !== selection.to 来判断是否有选择,如果有则不显示补全。

**情况二:光标在代码块中**。代码块的补全应该由代码编辑器处理,不是文本补全的职责。我通过检查 $from.node(-1)?.type.name === "codeBlock" 来判断是否在代码块中,如果是则不触发补全。

**情况三:文本太短**。如果文本只有一两个字符,上下文不足,补全质量会很差。我设置了最小字符数限制(默认 3 个字符),只有达到这个长度才触发补全。

**情况四:光标移动**。如果用户只是移动光标,没有输入文本,之前的补全建议应该清除。我在 apply 方法中检查是否有文本变化,如果没有但光标位置改变了,则清除补全状态。

**情况五:补全相关的操作**。用户按 Tab 接受补全时,会向文档中插入文本,这也会触发 apply 方法。但这种变化不应该触发新的补全请求,否则会陷入循环。我通过检查事务的元数据(ai-completion-accept)来排除这种情况。

### 技术优势

这个方案的优势主要体现在:

**第一,用户体验无缝**。补全建议实时显示在光标位置,按 Tab 接受,按 Esc 取消,完全符合用户习惯,不打断写作流程。而且视觉提示清晰(灰色斜体),既能引起注意又不干扰阅读。

**第二,性能优化到位**。500ms 的防抖机制大幅减少了 API 调用次数。在实际测试中,如果用户以正常速度输入,大约每 10 个字符才会触发一次 API 调用。而且双重位置校验避免了无效的状态更新和 DOM 渲染,性能开销很小。

**第三,稳定性高**。通过完善的边界检查和错误处理,即使在 API 失败或网络异常的情况下,编辑器仍然能正常工作。补全功能是辅助性的,不应该影响核心的编辑体验,我的实现做到了这一点。

**第四,代码结构清晰**。整个实现高度模块化,状态管理、渲染、API 调用各司其职。而且通过 ProseMirror 的插件机制,补全功能完全独立,可以轻松开启或关闭,不影响其他编辑器功能。

---

## 第二部分:RAG 向量索引系统 - 语义感知的智能分块

### 问题背景

实现 RAG 问答的核心是向量索引系统,需要把博客文章转换成向量并存储起来。但这里有个关键问题:文章内容往往很长,一篇技术文章可能有几千甚至上万字符,而 Embedding 模型对单次输入有严格的长度限制。

我使用的是 Ollama 的 nomic-embed-text 模型,它完全免费,可以本地运行,但有个硬性限制:单次输入不能超过 800 字符。如果直接把整篇文章送进去,会直接报错。

最直观的想法是按固定字符数硬切,比如每 800 字符切一段。但这样做会带来严重问题:

**第一,语义被破坏**。硬切可能把一个句子从中间切断,甚至把一个单词切开,导致分块内容语义不完整。在向量检索时,这种不完整的片段匹配度会很低,影响检索质量。

**第二,上下文丢失**。相邻的两个块之间没有任何联系,边界信息完全丢失。如果某个重要概念恰好在块的边界处,就会被割裂,检索时找不到完整的信息。

**第三,结构信息损失**。Markdown 文章有段落结构、标题层级等信息,硬切会完全忽略这些结构,导致分块毫无逻辑可言。

更复杂的是,即使按照语义分块,仍然可能出现超过 800 字符的块。比如一个段落本身就有 1000 字符,这种情况必须进一步处理,否则无法生成 embedding。

### 核心思路

我的解决思路是设计一个**三级语义感知分块 + 双重限制保护 + 多向量存储**的策略。

**第一级是按段落分割**。我使用 `\n\n+` 正则表达式匹配段落边界,因为 Markdown 中两个或多个换行符表示段落分隔。这样分出来的每个块都是完整的段落,保持了最基本的语义单元。段落是文章的最小独立意义单元,一个段落通常表达一个完整的观点。

**第二级是按句子分割**。如果单个段落太长,超过了限制,就需要进一步分割。我使用 `[。！？\n]` 正则表达式匹配句子边界。这样即使长段落被拆分,每个块仍然包含完整的句子,不会出现句子被截断的情况。

**第三级是严格的大小控制**。在前两级的语义分割基础上,我实现了双重限制检查:一是 token 数限制(默认 300 tokens),二是字符数硬限制(800 字符)。两个条件只要有一个超标,就触发分块保存。这确保了每个块都不会超过 Ollama 的限制。

**重叠策略**。为了避免边界信息丢失,我在相邻块之间保留 50 字符的重叠。具体做法是,保存当前块时,记录末尾的 50 个字符,下一个块从这 50 个字符开始构建。这样边界附近的信息会在两个块中都出现,提高了检索的连续性。

**多向量存储**。即使经过三级分块,仍然可能出现超过 800 字符的块(虽然概率很低,但必须处理)。对于这种情况,我不再尝试语义分割,而是直接按 800 字符硬切,为每个子块生成独立的 embedding,并分别存储到向量数据库中。这样虽然会产生多个向量,但保证了所有信息都被索引,不会丢失。

整个分块流程是这样的:

1. 先按段落分割整篇文章
2. 遍历每个段落,检查是否超过限制(token 或字符)
3. 如果段落不超限,尝试累加到当前块中
4. 如果累加后超限,保存当前块,并保留 50 字符重叠
5. 如果单个段落就超限,按句子进一步分割
6. 如果按句子分割后仍超限,按字符硬切(最后手段)
7. 最后进行全面验证,确保所有块都满足限制

### 技术难点与解决方案

#### 难点一:Token 估算的准确性

Token 估算是分块策略的基础,但中英文混合文本的 token 估算非常不准确。OpenAI 的 token 计算规则是:英文按子词(subword)切分,一个单词可能是 1-2 个 tokens;中文按字符切分,一个汉字通常是 1.5-2 个 tokens。

如果估算不准,可能导致两个问题:估算偏小,实际 token 数超限,API 调用失败;估算偏大,分块过于碎片化,降低检索质量。

我的解决方案是实现了一个粗略但实用的估算函数:

```typescript
function estimateTokens(text: string): number {
  const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length;
  const englishWords = text
    .split(/\s+/)
    .filter((w) => /[a-zA-Z]/.test(w)).length;
  return Math.ceil(chineseChars * 1.5 + englishWords * 1.3);
}
```

这个函数分别统计中文字符数和英文单词数,按照经验值计算 token 数。虽然不是完全准确,但误差在可接受范围内。更重要的是,我实现了双重检查机制:既检查 token 数,也检查字符数。字符数是硬限制,绝对不能超过 800,这确保了即使 token 估算有偏差,也不会导致 API 调用失败。

#### 难点二:语义分块与硬限制的平衡

这是整个分块策略最核心的矛盾:语义分块希望保持完整性,可能产生较大的块;但 Ollama 有 800 字符的硬限制,必须严格遵守。

最开始我只做了语义分块,没有严格控制大小,结果在索引某些文章时,生成的块超过了 800 字符,导致 embedding 生成失败。后来我改进了策略,在语义分块的每一步都进行大小检查。

具体来说,我设计了一个渐进式的分块策略:

1. **第一优先级:按段落**。如果段落不超限,优先保持段落完整
2. **第二优先级:按句子**。如果段落超限,按句子分割,保持句子完整
3. **第三优先级:硬切**。如果句子仍超限,按字符硬切,这是最后手段

在每个优先级都有双重检查:既检查 token 数,也检查字符数。只要有一个超限,就降级到下一个优先级。这样确保了在保持语义完整性的同时,严格遵守大小限制。

#### 难点三:多向量的管理和检索

当一个块超过 800 字符被硬切为多个子块时,会生成多个 embedding 向量。这些向量需要正确关联和管理,否则检索时会出问题。

我的解决方案是设计了一个分层的 ID 结构:

- 主块 ID: `postId_chunkIndex`(如 `post123_0`)
- 子块 ID: `postId_chunkIndex_subChunkIndex`(如 `post123_0_0`, `post123_0_1`)

在 metadata 中,我标记了 `isSubChunk: true` 和 `subChunkIndex`,这样在检索时可以识别子块。而且所有子块都共享同一个 `chunkIndex`,通过这个字段可以关联同一个语义块的所有子块。

在存储时,每个子块都是独立的向量记录,这样在向量相似度搜索时,每个子块都能被单独匹配。如果某个子块的语义与查询高度相关,它会被检索出来,不会因为是子块而被忽略。

#### 难点四:重叠区域的精确控制

重叠策略看似简单,实际实现时有很多细节需要注意。最大的问题是如何在分块逻辑中正确插入重叠。

如果在保存当前块后,直接把下一个段落加入新块,就没有重叠。如果在保存前就把重叠加到下一个块,逻辑会非常复杂,容易出错。

我的解决方案是在保存当前块时,记录末尾的 50 个字符(通过 `slice(-50)` 实现),然后在构建下一个块时,先加入这 50 个字符,再加入新内容。这样逻辑清晰,而且确保了重叠区域的连续性。

还有一个细节是边界处理:如果当前块少于 50 字符,取 `Math.min(overlap, currentChunk.length)`,避免越界。如果已经是最后一个块,不需要重叠,直接结束。

#### 难点五:最终验证和异常处理

即使经过严格的分块逻辑,仍然可能因为估算误差或极端情况产生超限的块。所以我在分块流程的最后加入了一个全面验证步骤:

遍历所有生成的块,检查字符数和 token 数(允许 20% 的估算误差)。如果发现超限的块,记录警告日志,并进行硬切处理。虽然这种情况理论上不应该发生,但作为最后的防护,确保系统的鲁棒性。

在 embedding 生成阶段,我还实现了重试机制:最多重试 3 次,每次间隔 1 秒,超时 30 秒。如果 Ollama 服务临时不可用或超时,重试机制可以提高成功率。

### 技术优势

这个向量索引系统的优势主要体现在:

**第一,语义完整性**。通过三级语义感知分块,绝大部分块都是完整的段落或句子,保持了语义的连贯性。在检索时,匹配到的片段都是有意义的完整内容,而不是被截断的碎片。

**第二,技术合规性**。严格遵守 Ollama 的 800 字符限制,通过双重检查(token + 字符)和多级分块策略,确保所有块都能成功生成 embedding,不会出现 API 调用失败的情况。

**第三,信息保留性**。通过重叠策略和多向量存储,确保了边界信息不丢失,长文本信息不丢失。即使是跨块的信息,也能通过重叠区域或多个子块被检索到。

**第四,性能优化**。通过批量处理、重试机制、限流保护等措施,在保证质量的同时,提升了索引构建的效率和稳定性。在实际测试中,索引一篇 5000 字的文章只需要 2-3 秒,性能完全可接受。

---

## 第三部分:流式输出实现 - SSE 驱动的实时交互体验

### 问题背景

在实现 RAG 问答功能时,我面临一个用户体验的问题:AI 生成回答需要时间,短则几秒,长则十几秒。如果采用传统的同步方式,用户点击提问后,界面会冻结,只能看到一个转圈的加载动画,一直等到回答完全生成完才一次性显示出来。

这种体验非常差,主要有三个问题:

**第一,等待焦虑**。用户不知道 AI 是在思考还是卡住了,只能干等,容易产生焦虑情绪。特别是当问题比较复杂,生成时间较长时,用户可能以为系统出错了,重复点击或者直接关闭页面。

**第二,缺乏即时反馈**。AI 其实是逐步生成回答的,但用户看不到这个过程,感觉不到进展,缺乏交互感。这与我们日常使用 ChatGPT 等现代 AI 产品的体验差距很大。

**第三,感知延迟长**。虽然 AI 可能在 1 秒后就开始输出内容,但用户要等 10 秒才能看到完整回答,首字节时间(TTFB)很长,感知延迟非常明显。

我需要的是类似 ChatGPT 的流式输出效果:用户提问后,AI 的回答逐字逐句地实时显示出来,像打字机一样。这样用户能立即感受到反馈,看到回答的生成过程,用户体验会好很多。

但实现流式输出有几个技术挑战:

**第一,需要保持长连接**。传统的 HTTP 请求是一问一答,服务器返回响应后连接就关闭了。而流式输出需要在一个请求中持续推送数据,连接要保持打开状态。

**第二,数据传输格式**。服务器要如何逐块推送数据?前端要如何接收和解析这些数据块?需要一个标准的协议来规范数据格式。

**第三,状态管理**。流式过程中需要传递多种信息:来源文章、回答片段、完成信号、错误信息等。这些不同类型的数据如何区分和处理?

**第四,异步复杂性**。流式输出涉及多个异步操作:向量检索、Kimi API 流式调用、SSE 推送、前端接收等。如何协调这些异步操作,确保数据正确传递?

### 核心思路

我的解决思路是采用 **SSE(Server-Sent Events) + ReadableStream + 回调驱动**的架构。

整体数据流是这样的:

1. 前端发送一个 POST 请求到 `/api/ai/rag/stream`
2. 后端创建一个 ReadableStream,保持连接打开
3. 执行向量检索(同步完成),立即通过 SSE 发送来源信息(sources 事件)
4. 调用 Kimi API 的流式模式,Kimi 逐块返回文本
5. 每收到一个文本块,就通过 SSE 发送给前端(chunk 事件)
6. 前端使用 ReadableStream.getReader() 读取流式数据,实时解析 SSE 格式
7. 根据事件类型更新 React 状态:sources 更新来源,chunk 追加文本,complete 标记完成
8. 回答生成完成后,发送 complete 事件,关闭连接

这个方案的关键是理解 SSE 的本质:**一个请求,多次推送**。整个过程只有一个 HTTP 请求,但服务器可以在这个请求中持续推送数据,直到显式关闭连接。

SSE 的数据格式非常简单:

```
event: 事件类型
data: JSON数据

```

每个事件由 `event` 和 `data` 两行组成,用两个换行符分隔。前端通过解析这个格式,就能区分不同类型的数据。

在后端实现上,我使用 Next.js 的 API Routes 创建 ReadableStream:

```typescript
const stream = new ReadableStream({
  async start(controller) {
    // 向量检索
    // ...

    // 发送来源信息
    controller.enqueue(
      `event: sources\ndata: ${JSON.stringify({ sources })}\n\n`
    );

    // 流式调用 Kimi
    await ragQueryStream(question, options, {
      onChunk: (chunk) => {
        controller.enqueue(
          `event: chunk\ndata: ${JSON.stringify({ chunk })}\n\n`
        );
      },
      onComplete: (result) => {
        controller.enqueue(
          `event: complete\ndata: ${JSON.stringify(result)}\n\n`
        );
        controller.close();
      },
    });
  },
});

return new Response(stream, {
  headers: {
    "Content-Type": "text/event-stream",
    Connection: "keep-alive",
  },
});
```

在前端实现上,我使用 fetch API 配合 ReadableStream 读取流式数据:

```typescript
const response = await fetch("/api/ai/rag/stream", {
  method: "POST",
  body: JSON.stringify({ question }),
});

const reader = response.body.getReader();
const decoder = new TextDecoder();
let buffer = "";

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  buffer += decoder.decode(value, { stream: true });
  const events = buffer.split("\n\n");
  buffer = events.pop() || "";

  for (const event of events) {
    // 解析 SSE 格式
    const [eventLine, dataLine] = event.split("\n");
    const eventType = eventLine.replace("event: ", "");
    const data = JSON.parse(dataLine.replace("data: ", ""));

    if (eventType === "sources") {
      setSources(data.sources);
    } else if (eventType === "chunk") {
      setAnswer((prev) => prev + data.chunk);
    } else if (eventType === "complete") {
      setLoading(false);
    }
  }
}
```

### 技术难点与解决方案

#### 难点一:SSE 数据的正确解析

SSE 的数据是按块到达的,一个块可能包含多个事件,也可能一个事件被分成多个块。如果直接按块处理,会出现数据不完整或解析错误的问题。

我的解决方案是使用 buffer 缓冲机制:

1. 维护一个 buffer 字符串,累积接收到的数据
2. 每次收到新数据,追加到 buffer 中
3. 用 `\n\n` 分割 buffer,得到完整的事件列表
4. 最后一个元素可能是不完整的事件,保留在 buffer 中
5. 处理所有完整的事件,不完整的留到下次处理

这样确保了每次处理的都是完整的事件,不会因为数据分块而出错。

#### 难点二:Kimi API 流式调用的封装

Kimi API 使用的是 OpenAI SDK,流式模式需要设置 `stream: true`,然后通过 `for await...of` 循环读取数据块。但这个流式读取是异步的,如何与 SSE 推送协调起来是个问题。

我的解决方案是使用回调函数来桥接:

在 `aiClient.chatStream` 方法中,我接受一个 `onChunk` 回调参数。每次从 Kimi API 收到数据块,就调用这个回调,将数据块传递出去。

```typescript
async chatStream(messages, options, onChunk) {
  const stream = await this.client.chat.completions.create({
    stream: true,
    // ...其他参数
  });

  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content || '';
    if (content) {
      onChunk?.(content); // 调用回调
    }
  }
}
```

在 RAG 模块中,我设计了一套完整的回调接口:

```typescript
export interface RAGStreamCallbacks {
  onSources?: (sources) => void; // 来源信息
  onChunk?: (chunk: string) => void; // 文本块
  onComplete?: (result) => void; // 完成信号
  onError?: (error) => void; // 错误
}
```

在 API 路由中,我将这些回调连接到 SSE 推送:

```typescript
onChunk: (chunk) => {
  controller.enqueue(`event: chunk\ndata: ${JSON.stringify({chunk})}\n\n`);
},
```

这样就形成了一个完整的数据流:Kimi API → 回调函数 → SSE 推送 → 前端接收。

#### 难点三:异步错误的正确传递

流式过程中可能在任何阶段出错:向量检索失败、Kimi API 调用失败、网络中断等。这些错误需要正确传递给前端,让用户知道出了什么问题。

我的解决方案是在整个流式过程包裹 try-catch,并通过 error 事件传递错误:

```typescript
try {
  // 向量检索
  // ...

  // 流式调用 Kimi
  await ragQueryStream(question, options, {
    onChunk: (chunk) => {
      controller.enqueue(
        `event: chunk\ndata: ${JSON.stringify({ chunk })}\n\n`
      );
    },
    onError: (error) => {
      controller.enqueue(
        `event: error\ndata: ${JSON.stringify({ error: error.message })}\n\n`
      );
      controller.close();
    },
  });
} catch (error) {
  controller.enqueue(
    `event: error\ndata: ${JSON.stringify({ error: error.message })}\n\n`
  );
  controller.close();
}
```

前端接收到 error 事件后,显示错误提示,停止加载动画。

#### 难点四:来源信息的提前返回

在实现 RAG 问答时,我发现了一个用户体验的优化点:向量检索是同步完成的,非常快(几十毫秒),而 AI 生成回答需要时间(几秒)。

如果等到 AI 生成完成后,再一起返回来源信息和回答,用户在等待期间看不到任何信息。更好的做法是:检索完成后,立即返回来源信息,让用户可以提前看到参考文章,然后再显示 AI 的回答。

我的实现是在流式调用开始前,先发送 sources 事件:

```typescript
// 1. 向量检索(同步完成,很快)
const searchResults = await vectorStore.search(queryVector, options);
const sources = searchResults.map((r) => ({
  postId: r.metadata.postId,
  title: r.metadata.title,
  // ...
}));

// 2. 立即发送来源信息(用户可以提前看到)
controller.enqueue(`event: sources\ndata: ${JSON.stringify({ sources })}\n\n`);

// 3. 然后开始流式生成回答
await ragQueryStream(question, options, {
  onChunk: (chunk) => {
    controller.enqueue(`event: chunk\ndata: ${JSON.stringify({ chunk })}\n\n`);
  },
});
```

这样用户在 AI 开始生成回答前,就能看到来源文章列表,体验更流畅。

### 技术优势

流式输出方案的优势主要体现在:

**第一,用户体验大幅提升**。从用户点击提问到看到第一个字符,时间从 10 秒缩短到 1 秒以内,感知延迟降低了 90%。而且逐字显示的打字机效果,让用户感觉 AI 在实时思考和回答,交互感很强。

**第二,等待焦虑消除**。用户可以实时看到回答的生成过程,知道 AI 正在工作,不会产生"是不是卡住了"的疑虑。即使回答很长,用户也能边看边等,而不是干等。

**第三,可中断性**。虽然目前没有实现,但流式架构天然支持中断功能。用户如果觉得回答不对,可以随时点击停止按钮,中断流式请求,不用等到全部生成完。这在同步方式下是做不到的。

**第四,技术标准化**。SSE 是 HTML5 标准,浏览器原生支持,兼容性好,不需要额外的库。而且协议简单,易于调试和扩展。相比 WebSocket,SSE 单向推送的特点更适合这个场景,实现也更简单。

---

## 第四部分:Prompt 工程与降级机制

### 问题背景

在实现 RAG 问答时,我发现 AI 的回答质量非常依赖 Prompt 的设计。同样的检索结果,不同的 Prompt 会导致完全不同的回答质量。

最开始我用的是一个简单的 Prompt:"根据以下内容回答问题:\n{context}\n\n问题:{question}"。测试时发现几个问题:

**第一,回答过于简短**。AI 倾向于给出一两句话的简单回答,没有展开,用户得不到充分的信息。

**第二,不引用来源**。回答中不会提到信息来自哪篇文章,用户无法追溯来源,降低了可信度。

**第三,容易"发挥创作"**。当检索到的内容不完全匹配问题时,AI 会根据自己的知识生成回答,而不是明确说明"检索到的内容不足以回答这个问题"。

另外还有一个技术风险:RAG 系统依赖 Ollama 服务来生成 embedding 向量。如果 Ollama 服务没有启动或者出现故障,整个 RAG 功能就不可用了,用户会看到错误提示,体验很差。

### 核心思路

针对 Prompt 质量问题,我设计了一个**结构化 Prompt 模板系统**,通过明确的指令和约束来规范 AI 的回答行为。

我的 Prompt 设计分为三个部分:

**第一部分是角色定位**:明确告诉 AI,你是一个基于知识库的问答助手,你的职责是根据提供的上下文回答问题,而不是发挥创作。

```typescript
export const RAG_SYSTEM_MESSAGE = `你是一个专业的技术博客问答助手。你的任务是根据提供的文章内容来回答用户的问题。

重要规则:
1. 只根据提供的文章内容回答,不要编造信息
2. 如果文章内容不足以回答问题,明确说明
3. 尽可能详细地回答,包含相关的技术细节
4. 如果合适,引用具体的文章标题
5. 使用清晰的 Markdown 格式`;
```

**第二部分是上下文提供**:将检索到的文章片段以结构化的方式提供给 AI,每个片段都标注了来源文章标题,用分隔线隔开。

```typescript
export function buildRAGPrompt(context: string, question: string): string {
  return `以下是相关的文章内容:

${context}

---

用户问题: ${question}

请根据上述文章内容回答问题。`;
}
```

**第三部分是约束条件**:在 System Message 中明确列出 5 条规则,规范 AI 的回答行为,避免常见问题。

针对降级机制,我实现了**服务可用性检查 + 降级策略**:

1. 在调用 RAG 功能前,先检查 Ollama 服务是否可用
2. 发送一个轻量级的测试请求到 Ollama 的 `/api/tags` 接口
3. 设置 2 秒超时,如果超时或返回错误,判定为不可用
4. 如果 Ollama 不可用,返回友好的错误提示,告诉用户需要启动 Ollama 服务

```typescript
async function isOllamaAvailable(): Promise<boolean> {
  try {
    const baseUrl = process.env.OLLAMA_BASE_URL || "http://localhost:11434";
    const response = await fetch(`${baseUrl}/api/tags`, {
      method: "GET",
      signal: AbortSignal.timeout(2000),
    });
    return response.ok;
  } catch {
    return false;
  }
}
```

在 RAG 查询函数的开头,先进行可用性检查:

```typescript
export async function ragQuery(question: string, options: RAGOptions = {}) {
  const ollamaAvailable = await isOllamaAvailable();

  if (!ollamaAvailable) {
    throw new Error("RAG 服务不可用:请确保 Ollama 服务已启动(默认端口 11434)");
  }

  // ... 后续 RAG 逻辑
}
```

### 技术难点与解决方案

#### 难点一:Prompt 的迭代优化

Prompt 工程是一个迭代优化的过程,很难一次就写出完美的 Prompt。我的方法是:

1. 准备一组测试问题,涵盖不同类型:直接匹配、需要推理、信息不足等
2. 用不同的 Prompt 测试,对比回答质量
3. 分析问题:回答是否准确、是否详细、是否引用来源、是否有幻觉
4. 针对性地修改 Prompt,添加或调整约束条件
5. 重新测试,直到满意为止

最终我的 Prompt 经过 5 次迭代,从最初的一句话变成了现在的结构化模板,回答质量有了明显提升。

#### 难点二:上下文的组织方式

检索到的多个文章片段如何组织成上下文也很关键。我尝试过两种方式:

**方式一:直接拼接**。把所有片段简单拼接在一起,用换行符分隔。问题是 AI 分不清哪个片段来自哪篇文章,引用来源时会很模糊。

**方式二:标注来源**。每个片段前加上 `[文章标题]`,用分隔线 `---` 隔开不同片段。这样 AI 能清楚地知道每个信息的来源,引用时会更准确。

我最终选择了方式二,上下文格式如下:

```
[React Hooks 详解]
React Hooks 是 React 16.8 引入的新特性...

---

[深入理解 useEffect]
useEffect 是最常用的 Hook 之一...

---

[自定义 Hook 最佳实践]
自定义 Hook 可以复用状态逻辑...
```

#### 难点三:错误提示的友好性

技术错误信息通常很晦涩,不适合直接展示给用户。比如 Ollama 不可用时,原始错误可能是 "ECONNREFUSED" 或 "fetch failed",用户看了一头雾水。

我的做法是在所有面向用户的错误提示中,都用通俗的语言解释问题,并给出解决方案:

- "RAG 服务不可用:请确保 Ollama 服务已启动(默认端口 11434)"
- "未找到相关内容,请尝试调整问题或确保文章已建立索引"
- "向量索引构建失败:请检查文章内容格式是否正确"

这样用户即使不懂技术,也能知道问题是什么,该怎么解决。

### 技术优势

Prompt 工程和降级机制的优势主要体现在:

**第一,回答质量稳定**。通过结构化的 Prompt 模板,AI 的回答质量更稳定,不会出现过于简短或发挥创作的问题。而且引用来源明确,提升了可信度。

**第二,容错性强**。通过服务可用性检查和友好的错误提示,即使 Ollama 服务不可用,用户也能得到清晰的反馈,知道如何解决问题,而不是看到一个技术错误信息。

**第三,可维护性好**。Prompt 模板独立管理,修改 Prompt 不需要改动业务代码。而且所有 Prompt 都有详细的注释,说明了设计意图和约束条件,方便后续优化。

---

## 可能的追问及回答

### Q1: AI Tab 补全的防抖时间为什么选择 500ms?

A: 这是一个平衡用户体验和性能开销的经验值。我测试了几个不同的时间:

- **300ms**: 响应很快,但用户快速输入时仍然会触发大量 API 调用,而且很多建议还没显示出来用户就继续输入了,实际利用率很低。
- **500ms**: 正常打字速度下,用户暂停 500ms 通常意味着在思考下一句话,这时显示建议正好。而且 500ms 的延迟用户基本感知不到。
- **1000ms**: 虽然 API 调用更少,但延迟太长,用户暂停 1 秒钟的情况比较少,建议出现的时机往往是用户已经继续输入了。

最终选择 500ms 是因为它在大部分情况下能捕捉到用户的暂停思考时刻,既不会过于频繁调用 API,也不会让用户感觉建议总是慢半拍。

### Q2: 为什么不直接用 EventSource API 而要自己解析 SSE?

A: 这是个好问题。EventSource 确实是浏览器提供的标准 SSE 客户端,但它有两个限制:

**第一,只支持 GET 请求**。EventSource 不支持 POST 请求,无法在请求体中传递复杂的参数。而 RAG 问答需要传递问题、过滤条件等多个参数,用 GET 请求的 URL 参数不够灵活,而且有长度限制。

**第二,无法自定义请求头**。我的 API 需要在请求头中传递一些元信息,比如 Content-Type 等,EventSource 不支持自定义请求头。

所以我选择了 fetch + ReadableStream 的方式,虽然需要手动解析 SSE 格式,但换来了更大的灵活性。而且 SSE 格式很简单,解析代码也不复杂。

如果未来需要自动重连等 EventSource 的特性,可以考虑用 POST 请求 + EventSource polyfill,但目前手动实现已经够用了。

### Q3: 向量分块时为什么选择 50 字符的重叠而不是更多?

A: 重叠的大小需要平衡信息冗余和存储开销:

- **重叠太小(如 20 字符)**: 可能无法完整保留边界的语义信息,特别是中文的话,20 字符可能只是半个句子,起不到连接作用。
- **重叠太大(如 100 字符)**: 相邻块之间重复内容太多,增加了存储和检索的开销,而且可能导致同一个信息被多次检索到,影响结果的多样性。
- **50 字符**: 对于中文来说,50 字符大约是 2-3 个完整的句子,足以保留边界的语义信息。对于英文来说,50 字符大约是 8-10 个单词,也能覆盖关键短语。

而且 50 这个数字是 800(最大块大小)的约 6%,重复率不高,存储开销可接受。在实际测试中,50 字符的重叠既保证了语义连贯性,又不会造成过多冗余。

### Q4: 为什么向量索引使用 Ollama 而不是 OpenAI 的 embedding API?

A: 这是基于成本和隐私的考虑:

**第一,成本**。OpenAI 的 embedding API 虽然质量很好,但需要付费,而且是按 token 数计费。如果索引大量文章,成本会比较高。Ollama 的 nomic-embed-text 模型完全免费,虽然效果可能稍差,但对于博客文章的语义检索已经够用。

**第二,隐私**。Ollama 是本地运行的,文章内容不需要上传到外部服务,完全掌握在自己手中,适合对隐私有要求的场景。OpenAI 的 API 需要把数据发送到他们的服务器,虽然有隐私协议,但终究是第三方服务。

**第三,依赖**。Ollama 可以在本地运行,不依赖外部服务,即使没有网络也能工作。OpenAI API 需要网络连接,而且可能受到 API 限流、服务中断等因素影响。

当然,如果追求最好的检索效果,或者文章主要是英文,可以考虑切换到 OpenAI 的 text-embedding-3-small 模型。我的架构设计是支持模型切换的,只需要修改环境变量即可。

### Q5: 流式输出的回答如果中断了,如何处理?

A: 流式输出确实可能因为网络问题或服务错误而中断,我的处理策略是:

**在前端**:

1. 监听 ReadableStream 的 done 状态,如果 done 为 true 但没有收到 complete 事件,判定为异常中断
2. 显示错误提示:"回答生成中断,请重试"
3. 保留已经生成的部分内容,让用户能看到已生成的信息,而不是全部丢失

**在后端**:

1. 使用 try-catch 包裹整个流式过程,捕获所有可能的异常
2. 发生异常时,通过 error 事件发送错误信息给前端
3. 确保在任何情况下都调用 controller.close() 关闭流

**重试机制**:
目前没有实现自动重试,因为流式过程可能已经生成了部分内容,自动重试会导致内容重复。我选择让用户手动重试,用户可以根据已生成的内容决定是否需要重新提问。

如果未来要实现自动重试,可以考虑在后端记录流式过程的检查点,中断时从检查点恢复,但这会增加实现复杂度。对于博客问答这个场景,手动重试已经够用。

### Q6: RAG 问答的检索结果数量为什么默认是 5?

A: 这是经过测试后的平衡选择:

**检索太少(如 3 个)**: 可能遗漏重要信息,特别是当问题涉及多个方面时,3 个块可能覆盖不全。而且有些块的匹配度可能不高,实际有效信息更少。

**检索太多(如 10 个)**: 会带来几个问题:

1. 上下文太长,超过 Kimi API 的 token 限制,或者影响生成速度
2. 噪音太多,相似度较低的块可能包含不相关的信息,反而干扰 AI 的判断
3. 来源文章太多,用户看不过来,降低了可读性

**5 个块**:

1. 通常来自 2-3 篇不同的文章,足以覆盖问题的主要方面
2. 上下文长度适中,大约 1000-2000 tokens,不会超过 Kimi 的限制
3. 来源文章数量合理,用户能快速浏览
4. 在实际测试中,5 个块的回答质量和准确性都比较好

当然,用户可以通过 options.limit 参数自定义检索数量,但对于大部分场景,5 是个不错的默认值。

### Q7: 为什么补全建议只显示在光标位置,不支持多处显示?

A: 这是基于用户体验的考虑:

**第一,避免干扰**。如果在多个位置同时显示建议,界面会非常混乱,用户不知道应该关注哪个建议,反而增加了认知负担。单一位置的建议更清晰,不会分散注意力。

**第二,性能开销**。多处显示意味着需要为每个位置都调用 AI API,或者调用一次 API 生成多个位置的建议。前者会大幅增加 API 调用次数,后者会增加 API 的复杂度和响应时间。

**第三,符合直觉**。用户的注意力通常集中在光标位置,这是他正在编辑的地方。在其他位置显示建议,用户可能根本不会注意到,或者感到突兀。

**第四,技术复杂度**。ProseMirror 的 Decoration 机制虽然支持多个装饰,但管理多个补全建议的状态会非常复杂,需要跟踪每个建议的位置、内容、接受/取消操作等,容易出 bug。

综合考虑,单一位置的补全建议既简单又实用,满足了大部分写作场景的需求。
