# AI é›†æˆå®ç°æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
3. [ä¾èµ–å®‰è£…](#ä¾èµ–å®‰è£…)
4. [æ•°æ®åº“è¿ç§»](#æ•°æ®åº“è¿ç§»)
5. [æ ¸å¿ƒæ¨¡å—å®ç°](#æ ¸å¿ƒæ¨¡å—å®ç°)
6. [API å®ç°](#api-å®ç°)
7. [å‰ç«¯é›†æˆ](#å‰ç«¯é›†æˆ)
8. [æµ‹è¯•å’Œè°ƒè¯•](#æµ‹è¯•å’Œè°ƒè¯•)

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# Kimi ä½¿ç”¨ OpenAI å…¼å®¹çš„ SDK
npm install openai chromadb
npm install -D @types/node

# å®‰è£… Ollama (ç”¨äºæœ¬åœ° embedding)
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# æ‹‰å– embedding æ¨¡å‹
ollama pull nomic-embed-text
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

åœ¨ `.env.local` ä¸­æ·»åŠ :

```bash
# AI é…ç½® (Kimi - æœ‰å…è´¹é¢åº¦)
AI_PROVIDER=kimi
KIMI_API_KEY=sk-your-kimi-key-here
KIMI_BASE_URL=https://api.moonshot.cn/v1
KIMI_MODEL=moonshot-v1-32k

# å‘é‡æ•°æ®åº“ (Chroma - å…è´¹å¼€æº)
VECTOR_DB_PROVIDER=chroma
CHROMA_PATH=./data/chroma

# åµŒå…¥æ¨¡å‹ (Ollama - æœ¬åœ°å…è´¹)
EMBEDDING_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

### 3. è¿è¡Œæ•°æ®åº“è¿ç§»

```bash
npx prisma migrate dev --name add_ai_features
```

---

## ç¯å¢ƒé…ç½®

### å®Œæ•´ç¯å¢ƒå˜é‡åˆ—è¡¨

```bash
# ============================================
# AI æ¨¡å‹é…ç½® (Kimi - æœ‰å…è´¹é¢åº¦)
# ============================================
AI_PROVIDER=kimi
KIMI_API_KEY=sk-xxx
KIMI_BASE_URL=https://api.moonshot.cn/v1
KIMI_MODEL=moonshot-v1-32k  # moonshot-v1-8k | moonshot-v1-32k | moonshot-v1-128k

# ============================================
# å‘é‡æ•°æ®åº“é…ç½® (Chroma - å…è´¹å¼€æº)
# ============================================
VECTOR_DB_PROVIDER=chroma
CHROMA_PATH=./data/chroma

# ============================================
# åµŒå…¥æ¨¡å‹é…ç½® (Ollama - æœ¬åœ°å…è´¹)
# ============================================
EMBEDDING_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIMENSION=768

# ============================================
# åŠŸèƒ½å¼€å…³
# ============================================
ENABLE_AI_WRITING=true
ENABLE_RAG=true
ENABLE_AI_QA=true

# ============================================
# é™åˆ¶é…ç½®
# ============================================
AI_MAX_TOKENS=2000
AI_TEMPERATURE=0.7
RAG_MAX_RESULTS=5
```

---

## ä¾èµ–å®‰è£…

### å¿…éœ€ä¾èµ–

```bash
# AI å®¢æˆ·ç«¯ (Kimi ä½¿ç”¨ OpenAI SDKï¼Œå› ä¸º API å…¼å®¹)
npm install openai

# å‘é‡æ•°æ®åº“ (Chroma - å…è´¹å¼€æº)
npm install chromadb

# å·¥å…·åº“
npm install zod  # å·²æœ‰,ç”¨äºç±»å‹éªŒè¯
npm install dotenv  # ç¯å¢ƒå˜é‡ç®¡ç†
```

### å¯é€‰ä¾èµ–

```bash
# æœ¬åœ°æ¨¡å‹æ”¯æŒ
npm install ollama
```

### package.json æ›´æ–°

```json
{
  "dependencies": {
    "openai": "^4.20.0",
    "chromadb": "^1.8.0",
    "dotenv": "^16.3.1"
  }
}
```

---

## æ•°æ®åº“è¿ç§»

### 1. æ›´æ–° Prisma Schema

åœ¨ `prisma/schema.prisma` æ–‡ä»¶æœ«å°¾æ·»åŠ :

```prisma
// ============================================================================
// AI åŠŸèƒ½ç›¸å…³æ¨¡å‹
// ============================================================================

model PromptTemplate {
  id          String   @id @default(cuid())
  name        String
  description String?
  category    PromptCategory
  content     String
  variables   Json?
  isPublic    Boolean  @default(false)
  usageCount  Int      @default(0)
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  userId      String?
  user        User?    @relation(fields: [userId], references: [id], onDelete: Cascade)

  @@map("prompt_templates")
  @@index([category])
  @@index([userId])
}

enum PromptCategory {
  WRITING
  OPTIMIZATION
  QNA
  ANALYSIS
  CUSTOM
}

model AIConversation {
  id          String   @id @default(cuid())
  title       String?
  type        ConversationType
  context     Json?
  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  userId      String
  user        User    @relation(fields: [userId], references: [id], onDelete: Cascade)

  messages    AIMessage[]

  @@map("ai_conversations")
  @@index([userId])
  @@index([type])
}

model AIMessage {
  id             String   @id @default(cuid())
  role           MessageRole
  content        String
  metadata       Json?
  tokenCount     Int?
  createdAt      DateTime @default(now())

  conversationId String
  conversation   AIConversation @relation(fields: [conversationId], references: [id], onDelete: Cascade)

  @@map("ai_messages")
  @@index([conversationId])
}

enum MessageRole {
  USER
  ASSISTANT
  SYSTEM
}

enum ConversationType {
  WRITING_ASSISTANT
  RAG_QA
  CONTENT_OPTIMIZE
  GENERAL
}

model PostVectorIndex {
  id          String   @id @default(cuid())
  postId      String   @unique
  post        Post     @relation(fields: [postId], references: [id], onDelete: Cascade)

  vectorId    String?
  chunkCount  Int      @default(0)
  indexedAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  @@map("post_vector_indexes")
  @@index([postId])
}

// æ‰©å±• User æ¨¡å‹
model User {
  // ... ç°æœ‰å­—æ®µä¿æŒä¸å˜ ...

  // æ–°å¢å…³ç³»
  promptTemplates  PromptTemplate[]
  aiConversations  AIConversation[]
}

// æ‰©å±• Post æ¨¡å‹
model Post {
  // ... ç°æœ‰å­—æ®µä¿æŒä¸å˜ ...

  // æ–°å¢å…³ç³»
  vectorIndex      PostVectorIndex?
}
```

### 2. è¿è¡Œè¿ç§»

```bash
# ç”Ÿæˆè¿ç§»æ–‡ä»¶
npx prisma migrate dev --name add_ai_features

# ç”Ÿæˆ Prisma Client
npx prisma generate
```

### 3. åˆå§‹åŒ–é¢„è®¾ Prompt æ¨¡æ¿

åˆ›å»º `prisma/seed-ai.ts`:

```typescript
import { PrismaClient, PromptCategory } from "@prisma/client";

const prisma = new PrismaClient();

async function seedAIPrompts() {
  const prompts = [
    {
      name: "æ ‡é¢˜ç”Ÿæˆ",
      description: "æ ¹æ®æ–‡ç« å†…å®¹ç”ŸæˆSEOå‹å¥½çš„æ ‡é¢˜",
      category: PromptCategory.WRITING,
      content: `æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹,ç”Ÿæˆ3ä¸ªSEOå‹å¥½ä¸”å¸å¼•äººçš„æ ‡é¢˜:

å†…å®¹:
{{content}}

è¦æ±‚:
- é•¿åº¦æ§åˆ¶åœ¨10-15å­—
- åŒ…å«æ ¸å¿ƒå…³é”®è¯
- é£æ ¼: {{style}}
- è¿”å›JSONæ ¼å¼: ["æ ‡é¢˜1", "æ ‡é¢˜2", "æ ‡é¢˜3"]`,
      variables: {
        content: { type: "string", required: true, description: "æ–‡ç« å†…å®¹" },
        style: {
          type: "string",
          required: false,
          description: "æ ‡é¢˜é£æ ¼",
          default: "ä¸“ä¸š",
        },
      },
      isPublic: true,
    },
    {
      name: "æ‘˜è¦ç”Ÿæˆ",
      description: "è‡ªåŠ¨ç”Ÿæˆæ–‡ç« æ‘˜è¦",
      category: PromptCategory.WRITING,
      content: `ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦:

æ ‡é¢˜: {{title}}
å†…å®¹: {{content}}

è¦æ±‚:
- é•¿åº¦: 100-200å­—
- çªå‡ºæ ¸å¿ƒè§‚ç‚¹
- å¸å¼•è¯»è€…ç‚¹å‡»`,
      variables: {
        title: { type: "string", required: true },
        content: { type: "string", required: true },
      },
      isPublic: true,
    },
    {
      name: "æ ‡ç­¾å»ºè®®",
      description: "åŸºäºå†…å®¹æ¨èæ ‡ç­¾",
      category: PromptCategory.WRITING,
      content: `æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹,æ¨è5-8ä¸ªç›¸å…³æ ‡ç­¾:

æ ‡é¢˜: {{title}}
å†…å®¹: {{content}}

è¦æ±‚:
- æ ‡ç­¾ç®€æ´(2-4å­—)
- è¦†ç›–ä¸»è¦ä¸»é¢˜
- è¿”å›JSONæ•°ç»„æ ¼å¼`,
      variables: {
        title: { type: "string", required: true },
        content: { type: "string", required: true },
      },
      isPublic: true,
    },
  ];

  for (const prompt of prompts) {
    await prisma.promptTemplate.upsert({
      where: { name: prompt.name },
      update: prompt,
      create: prompt,
    });
  }

  console.log("AI Prompt æ¨¡æ¿åˆå§‹åŒ–å®Œæˆ");
}

seedAIPrompts()
  .catch(console.error)
  .finally(() => prisma.$disconnect());
```

è¿è¡Œ:

```bash
tsx prisma/seed-ai.ts
```

---

## æ ¸å¿ƒæ¨¡å—å®ç°

### 1. AI å®¢æˆ·ç«¯å°è£…

åˆ›å»º `src/lib/ai/client.ts`:

```typescript
import OpenAI from "openai";
import Anthropic from "@anthropic-ai/sdk";

export type AIProvider = "kimi" | "openai" | "claude" | "ollama";

export interface AIClient {
  chat(messages: ChatMessage[], options?: ChatOptions): Promise<ChatResponse>;
  embed(text: string | string[]): Promise<number[][]>;
}

export interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

export interface ChatOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
}

export interface ChatResponse {
  content: string;
  tokensUsed?: number;
}

/**
 * Kimi å®¢æˆ·ç«¯ (Moonshot AI)
 * ä½¿ç”¨ OpenAI SDKï¼Œå› ä¸º Kimi API å…¼å®¹ OpenAI æ ¼å¼
 */
class KimiClient implements AIClient {
  private client: OpenAI;

  constructor() {
    this.client = new OpenAI({
      apiKey: process.env.KIMI_API_KEY,
      baseURL: process.env.KIMI_BASE_URL || "https://api.moonshot.cn/v1",
    });
  }

  async chat(
    messages: ChatMessage[],
    options: ChatOptions = {}
  ): Promise<ChatResponse> {
    const response = await this.client.chat.completions.create({
      model: options.model || process.env.KIMI_MODEL || "moonshot-v1-32k",
      messages: messages.map((m) => ({
        role: m.role,
        content: m.content,
      })),
      temperature: options.temperature ?? 0.7,
      max_tokens: options.maxTokens ?? 2000,
    });

    return {
      content: response.choices[0]?.message?.content || "",
      tokensUsed: response.usage?.total_tokens,
    };
  }

  async embed(text: string | string[]): Promise<number[][]> {
    // Kimi ä¸æ”¯æŒ embeddingï¼Œä½¿ç”¨ Ollama
    return ollamaEmbed(text);
  }
}

/**
 * Ollama æœ¬åœ° Embedding (å…è´¹)
 */
async function ollamaEmbed(text: string | string[]): Promise<number[][]> {
  const texts = Array.isArray(text) ? text : [text];
  const baseUrl = process.env.OLLAMA_BASE_URL || "http://localhost:11434";
  const model = process.env.OLLAMA_EMBEDDING_MODEL || "nomic-embed-text";

  const embeddings: number[][] = [];

  for (const t of texts) {
    const response = await fetch(`${baseUrl}/api/embeddings`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ model, prompt: t }),
    });

    const data = await response.json();
    embeddings.push(data.embedding);
  }

  return embeddings;
}

class OpenAIClient implements AIClient {
  private client: OpenAI;

  constructor() {
    this.client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      baseURL: process.env.OPENAI_BASE_URL,
    });
  }

  async chat(
    messages: ChatMessage[],
    options: ChatOptions = {}
  ): Promise<ChatResponse> {
    const response = await this.client.chat.completions.create({
      model: options.model || process.env.OPENAI_MODEL || "gpt-4-turbo-preview",
      messages: messages.map((m) => ({
        role: m.role,
        content: m.content,
      })),
      temperature: options.temperature ?? 0.7,
      max_tokens: options.maxTokens ?? 2000,
    });

    return {
      content: response.choices[0]?.message?.content || "",
      tokensUsed: response.usage?.total_tokens,
    };
  }

  async embed(text: string | string[]): Promise<number[][]> {
    const texts = Array.isArray(text) ? text : [text];
    const response = await this.client.embeddings.create({
      model: process.env.EMBEDDING_MODEL || "text-embedding-3-small",
      input: texts,
    });

    return response.data.map((item) => item.embedding);
  }
}

class ClaudeClient implements AIClient {
  private client: Anthropic;

  constructor() {
    this.client = new Anthropic({
      apiKey: process.env.CLAUDE_API_KEY!,
    });
  }

  async chat(
    messages: ChatMessage[],
    options: ChatOptions = {}
  ): Promise<ChatResponse> {
    // Claude éœ€è¦ system æ¶ˆæ¯å•ç‹¬å¤„ç†
    const systemMessage = messages.find((m) => m.role === "system");
    const conversationMessages = messages.filter((m) => m.role !== "system");

    const response = await this.client.messages.create({
      model:
        options.model || process.env.CLAUDE_MODEL || "claude-3-opus-20240229",
      max_tokens: options.maxTokens ?? 2000,
      system: systemMessage?.content,
      messages: conversationMessages.map((m) => ({
        role: m.role === "assistant" ? "assistant" : "user",
        content: m.content,
      })),
    });

    const content = response.content[0];
    if (content.type !== "text") {
      throw new Error("Unexpected response type");
    }

    return {
      content: content.text,
      tokensUsed: response.usage.input_tokens + response.usage.output_tokens,
    };
  }

  async embed(text: string | string[]): Promise<number[][]> {
    // Claude ä¸ç›´æ¥æ”¯æŒåµŒå…¥,éœ€è¦ä½¿ç”¨å…¶ä»–æœåŠ¡
    throw new Error("Claude does not support embeddings directly");
  }
}

// å·¥å‚å‡½æ•°
export function createAIClient(provider?: AIProvider): AIClient {
  const providerName =
    provider || (process.env.AI_PROVIDER as AIProvider) || "kimi";

  switch (providerName) {
    case "kimi":
      return new KimiClient();
    case "openai":
      return new OpenAIClient();
    case "claude":
      return new ClaudeClient();
    default:
      throw new Error(`Unsupported AI provider: ${providerName}`);
  }
}

// å•ä¾‹
let aiClientInstance: AIClient | null = null;

export function getAIClient(): AIClient {
  if (!aiClientInstance) {
    aiClientInstance = createAIClient();
  }
  return aiClientInstance;
}
```

### 2. å‘é‡å­˜å‚¨å°è£…

åˆ›å»º `src/lib/vector/store.ts`:

```typescript
import { ChromaClient, Collection } from "chromadb";

export interface VectorStore {
  upsert(vectors: Vector[]): Promise<string[]>;
  search(
    queryVector: number[],
    options: SearchOptions
  ): Promise<SearchResult[]>;
  delete(ids: string[]): Promise<void>;
}

export interface Vector {
  id?: string;
  embedding: number[];
  metadata: Record<string, any>;
  document?: string;
}

export interface SearchOptions {
  limit?: number;
  filters?: Record<string, any>;
}

export interface SearchResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
  document?: string;
}

/**
 * Chroma å‘é‡å­˜å‚¨ (å¼€æºå…è´¹)
 */
class ChromaVectorStore implements VectorStore {
  private client: ChromaClient;
  private collection: Collection | null = null;
  private collectionName = "blog_posts";

  constructor() {
    const path = process.env.CHROMA_PATH || "./data/chroma";
    this.client = new ChromaClient({ path });
    this.initialize();
  }

  private async initialize() {
    try {
      this.collection = await this.client.getOrCreateCollection({
        name: this.collectionName,
      });
    } catch (error) {
      console.error("Failed to initialize Chroma collection:", error);
      throw error;
    }
  }

  async upsert(vectors: Vector[]): Promise<string[]> {
    if (!this.collection) {
      await this.initialize();
    }

    const ids = vectors.map((v, i) => v.id || `vec_${Date.now()}_${i}`);
    const embeddings = vectors.map((v) => v.embedding);
    const metadatas = vectors.map((v) => v.metadata);
    const documents = vectors.map((v) => v.document || "");

    await this.collection!.upsert({
      ids,
      embeddings,
      metadatas,
      documents,
    });

    return ids;
  }

  async search(
    queryVector: number[],
    options: SearchOptions = {}
  ): Promise<SearchResult[]> {
    if (!this.collection) {
      await this.initialize();
    }

    const results = await this.collection!.query({
      queryEmbeddings: [queryVector],
      nResults: options.limit || 5,
      where: options.filters,
    });

    return (results.ids[0] || []).map((id, i) => ({
      id: id as string,
      score: 1 - (results.distances?.[0]?.[i] || 0),
      metadata: (results.metadatas?.[0]?.[i] as Record<string, any>) || {},
      document: results.documents?.[0]?.[i] as string,
    }));
  }

  async delete(ids: string[]): Promise<void> {
    if (!this.collection) {
      await this.initialize();
    }

    await this.collection!.delete({ ids });
  }
}

// å·¥å‚å‡½æ•°
export function createVectorStore(): VectorStore {
  const provider = process.env.VECTOR_DB_PROVIDER || "chroma";

  switch (provider) {
    case "chroma":
      return new ChromaVectorStore();
    default:
      throw new Error(`Unsupported vector store: ${provider}`);
  }
}

// å•ä¾‹
let vectorStoreInstance: VectorStore | null = null;

export function getVectorStore(): VectorStore {
  if (!vectorStoreInstance) {
    vectorStoreInstance = createVectorStore();
  }
  return vectorStoreInstance;
}
```

### 3. æ–‡ç« åˆ†å—å·¥å…·

åˆ›å»º `src/lib/vector/chunker.ts`:

```typescript
export interface Chunk {
  content: string;
  index: number;
  metadata?: Record<string, any>;
}

/**
 * ä¼°ç®—æ–‡æœ¬çš„ token æ•°é‡(ç²—ç•¥ä¼°ç®—)
 */
function estimateTokens(text: string): number {
  // ä¸­æ–‡æŒ‰å­—ç¬¦æ•°,è‹±æ–‡æŒ‰å•è¯æ•°ä¼°ç®—
  const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length;
  const englishWords = text
    .split(/\s+/)
    .filter((w) => /[a-zA-Z]/.test(w)).length;
  // 1ä¸ªä¸­æ–‡å­—ç¬¦ â‰ˆ 1.5 tokens, 1ä¸ªè‹±æ–‡å•è¯ â‰ˆ 1.3 tokens
  return Math.ceil(chineseChars * 1.5 + englishWords * 1.3);
}

/**
 * å°†æ–‡ç« å†…å®¹åˆ†å—
 */
export function chunkPost(
  content: string,
  options: {
    maxTokens?: number;
    overlap?: number;
  } = {}
): Chunk[] {
  const maxTokens = options.maxTokens || 500;
  const overlap = options.overlap || 50;
  const chunks: Chunk[] = [];

  // æŒ‰æ®µè½åˆ†å‰²
  const paragraphs = content.split(/\n\n+/).filter((p) => p.trim());

  let currentChunk = "";
  let chunkIndex = 0;

  for (const para of paragraphs) {
    const paraTokens = estimateTokens(para);

    if (paraTokens > maxTokens) {
      // æ®µè½è¿‡é•¿,å…ˆä¿å­˜å½“å‰å—
      if (currentChunk.trim()) {
        chunks.push({
          content: currentChunk.trim(),
          index: chunkIndex++,
        });
        currentChunk = "";
      }

      // æŒ‰å¥å­åˆ†å‰²é•¿æ®µè½
      const sentences = para.split(/[ã€‚ï¼ï¼Ÿ\n]/).filter((s) => s.trim());
      let sentenceChunk = "";

      for (const sentence of sentences) {
        const sentenceTokens = estimateTokens(sentence);
        const chunkTokens = estimateTokens(sentenceChunk + sentence);

        if (chunkTokens > maxTokens && sentenceChunk) {
          chunks.push({
            content: sentenceChunk.trim(),
            index: chunkIndex++,
          });
          // ä¿ç•™é‡å éƒ¨åˆ†
          const overlapText = sentenceChunk.slice(-overlap);
          sentenceChunk = overlapText + sentence;
        } else {
          sentenceChunk += sentence;
        }
      }

      if (sentenceChunk.trim()) {
        chunks.push({
          content: sentenceChunk.trim(),
          index: chunkIndex++,
        });
      }
    } else {
      const chunkTokens = estimateTokens(currentChunk + para);

      if (chunkTokens > maxTokens && currentChunk) {
        chunks.push({
          content: currentChunk.trim(),
          index: chunkIndex++,
        });
        // ä¿ç•™é‡å éƒ¨åˆ†
        const overlapText = currentChunk.slice(-overlap);
        currentChunk = overlapText + para;
      } else {
        currentChunk += (currentChunk ? "\n\n" : "") + para;
      }
    }
  }

  // ä¿å­˜æœ€åä¸€ä¸ªå—
  if (currentChunk.trim()) {
    chunks.push({
      content: currentChunk.trim(),
      index: chunkIndex,
    });
  }

  return chunks;
}
```

### 4. RAG æ£€ç´¢å®ç°

åˆ›å»º `src/lib/ai/rag.ts`:

```typescript
import { getAIClient } from "./client";
import { getVectorStore } from "../vector/store";
import { prisma } from "../prisma";

export interface RAGOptions {
  limit?: number;
  filters?: {
    postId?: string;
    categoryId?: string;
    tags?: string[];
  };
  maxTokens?: number;
}

export interface RAGResponse {
  answer: string;
  sources: Array<{
    postId: string;
    title: string;
    excerpt: string;
    score: number;
  }>;
  tokensUsed?: number;
}

/**
 * RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ
 */
export async function ragQuery(
  question: string,
  options: RAGOptions = {}
): Promise<RAGResponse> {
  const aiClient = getAIClient();
  const vectorStore = getVectorStore();

  // 1. å°†é—®é¢˜å‘é‡åŒ–
  const [queryVector] = await aiClient.embed(question);

  // 2. æ„å»ºè¿‡æ»¤æ¡ä»¶
  const filters: Record<string, any> = {};
  if (options.filters?.categoryId) {
    filters.categoryId = options.filters.categoryId;
  }
  if (options.filters?.tags && options.filters.tags.length > 0) {
    filters.tags = { $in: options.filters.tags };
  }

  // 3. å‘é‡ç›¸ä¼¼åº¦æœç´¢
  const searchResults = await vectorStore.search(queryVector, {
    limit: options.limit || 5,
    filters: Object.keys(filters).length > 0 ? filters : undefined,
  });

  // 4. è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯
  const postIds = [...new Set(searchResults.map((r) => r.metadata.postId))];
  const posts = await prisma.post.findMany({
    where: { id: { in: postIds } },
    select: {
      id: true,
      title: true,
      slug: true,
      excerpt: true,
    },
  });

  const postMap = new Map(posts.map((p) => [p.id, p]));

  // 5. æ„å»ºä¸Šä¸‹æ–‡
  const context = searchResults
    .map((result) => {
      const post = postMap.get(result.metadata.postId);
      if (!post) return null;
      return `[${post.title}]\n${result.document}`;
    })
    .filter(Boolean)
    .join("\n\n---\n\n");

  // 6. æ„å»º Prompt
  const prompt = `åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

çŸ¥è¯†åº“å†…å®¹:
${context}

é—®é¢˜: ${question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”,å¹¶åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³æ¥æºã€‚`;

  // 7. è°ƒç”¨ LLM
  const response = await aiClient.chat(
    [
      {
        role: "system",
        content:
          "ä½ æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†åº“çš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜,å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·æ˜ç¡®è¯´æ˜ã€‚",
      },
      { role: "user", content: prompt },
    ],
    {
      maxTokens: options.maxTokens || 1000,
    }
  );

  // 8. æ„å»ºæ¥æºä¿¡æ¯
  const sources = searchResults.map((result) => {
    const post = postMap.get(result.metadata.postId);
    return {
      postId: result.metadata.postId,
      title: post?.title || "æœªçŸ¥",
      excerpt: result.document?.slice(0, 200) || "",
      score: result.score,
    };
  });

  return {
    answer: response.content,
    sources,
    tokensUsed: response.tokensUsed,
  };
}
```

---

## API å®ç°

### 1. å†…å®¹è¡¥å…¨ API

åˆ›å»º `src/app/api/ai/write/complete/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { getAIClient } from "@/lib/ai/client";

export async function POST(request: NextRequest) {
  try {
    // æƒé™éªŒè¯
    const session = await getServerSession(authOptions);
    if (!session?.user) {
      return NextResponse.json({ error: "æœªæˆæƒ" }, { status: 401 });
    }

    const body = await request.json();
    const { content, cursorPosition, context, style } = body;

    if (!content) {
      return NextResponse.json({ error: "å†…å®¹ä¸èƒ½ä¸ºç©º" }, { status: 400 });
    }

    // è·å–å…‰æ ‡å‰åçš„ä¸Šä¸‹æ–‡
    const beforeCursor = content.slice(0, cursorPosition);
    const afterCursor = content.slice(cursorPosition);
    const lastParagraph = beforeCursor.split("\n\n").pop() || "";

    // æ„å»º Prompt
    const prompt = `è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡,è¡¥å…¨å†…å®¹ã€‚åªè¿”å›è¡¥å…¨çš„éƒ¨åˆ†,ä¸è¦é‡å¤å·²æœ‰å†…å®¹ã€‚

å·²æœ‰å†…å®¹:
${lastParagraph}

åç»­å†…å®¹:
${afterCursor.slice(0, 100)}

é£æ ¼è¦æ±‚: ${style || "è‡ªç„¶æµç•…"}

è¯·è¡¥å…¨å†…å®¹:`;

    const aiClient = getAIClient();
    const response = await aiClient.chat([{ role: "user", content: prompt }], {
      maxTokens: 200,
      temperature: 0.7,
    });

    return NextResponse.json({
      suggestions: [
        {
          text: response.content.trim(),
          confidence: 0.9,
        },
      ],
      tokensUsed: response.tokensUsed,
    });
  } catch (error) {
    console.error("AI è¡¥å…¨é”™è¯¯:", error);
    return NextResponse.json({ error: "AI è¡¥å…¨å¤±è´¥" }, { status: 500 });
  }
}
```

### 2. å†…å®¹ç”Ÿæˆ API

åˆ›å»º `src/app/api/ai/write/generate/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { getAIClient } from "@/lib/ai/client";

export async function POST(request: NextRequest) {
  try {
    const session = await getServerSession(authOptions);
    if (!session?.user) {
      return NextResponse.json({ error: "æœªæˆæƒ" }, { status: 401 });
    }

    const body = await request.json();
    const { type, content, options } = body;

    if (!type || !content) {
      return NextResponse.json(
        { error: "ç±»å‹å’Œå†…å®¹ä¸èƒ½ä¸ºç©º" },
        { status: 400 }
      );
    }

    const aiClient = getAIClient();
    let prompt = "";

    switch (type) {
      case "title":
        prompt = `æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹,ç”Ÿæˆ${options?.count || 3}ä¸ªSEOå‹å¥½ä¸”å¸å¼•äººçš„æ ‡é¢˜:

å†…å®¹:
${content.slice(0, 2000)}

è¦æ±‚:
- é•¿åº¦æ§åˆ¶åœ¨10-15å­—
- åŒ…å«æ ¸å¿ƒå…³é”®è¯
- é£æ ¼: ${options?.style || "ä¸“ä¸š"}

è¯·è¿”å›JSONæ•°ç»„æ ¼å¼,ä¾‹å¦‚: ["æ ‡é¢˜1", "æ ‡é¢˜2", "æ ‡é¢˜3"]`;
        break;

      case "excerpt":
        prompt = `ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦:

å†…å®¹:
${content}

è¦æ±‚:
- é•¿åº¦: 100-200å­—
- çªå‡ºæ ¸å¿ƒè§‚ç‚¹
- å¸å¼•è¯»è€…ç‚¹å‡»`;
        break;

      case "tags":
        prompt = `æ ¹æ®ä»¥ä¸‹æ–‡ç« å†…å®¹,æ¨è5-8ä¸ªç›¸å…³æ ‡ç­¾:

å†…å®¹:
${content.slice(0, 2000)}

è¦æ±‚:
- æ ‡ç­¾ç®€æ´(2-4å­—)
- è¦†ç›–ä¸»è¦ä¸»é¢˜
- è¿”å›JSONæ•°ç»„æ ¼å¼`;
        break;

      default:
        return NextResponse.json({ error: "ä¸æ”¯æŒçš„ç±»å‹" }, { status: 400 });
    }

    const response = await aiClient.chat([{ role: "user", content: prompt }], {
      maxTokens: 500,
      temperature: 0.7,
    });

    // å°è¯•è§£æ JSON ç»“æœ
    let results: string[];
    try {
      results = JSON.parse(response.content);
    } catch {
      // å¦‚æœä¸æ˜¯ JSON,æŒ‰è¡Œåˆ†å‰²
      results = response.content
        .split("\n")
        .map((line) => line.replace(/^[-*â€¢]\s*/, "").trim())
        .filter(Boolean);
    }

    return NextResponse.json({
      results,
      tokensUsed: response.tokensUsed,
    });
  } catch (error) {
    console.error("AI ç”Ÿæˆé”™è¯¯:", error);
    return NextResponse.json({ error: "AI ç”Ÿæˆå¤±è´¥" }, { status: 500 });
  }
}
```

### 3. RAG é—®ç­” API

åˆ›å»º `src/app/api/ai/rag/query/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { ragQuery } from "@/lib/ai/rag";
import { prisma } from "@/lib/prisma";

export async function POST(request: NextRequest) {
  try {
    const session = await getServerSession(authOptions);
    if (!session?.user) {
      return NextResponse.json({ error: "æœªæˆæƒ" }, { status: 401 });
    }

    const body = await request.json();
    const { question, context, options } = body;

    if (!question) {
      return NextResponse.json({ error: "é—®é¢˜ä¸èƒ½ä¸ºç©º" }, { status: 400 });
    }

    // æ‰§è¡Œ RAG æŸ¥è¯¢
    const response = await ragQuery(question, {
      limit: options?.limit || 5,
      filters: context?.postId
        ? { postId: context.postId }
        : context?.categoryId
          ? { categoryId: context.categoryId }
          : undefined,
      maxTokens: options?.maxTokens || 1000,
    });

    // ä¿å­˜å¯¹è¯è®°å½•(å¯é€‰)
    let conversationId = context?.conversationId;
    if (!conversationId) {
      const conversation = await prisma.aIConversation.create({
        data: {
          userId: session.user.id,
          type: "RAG_QA",
          title: question.slice(0, 50),
        },
      });
      conversationId = conversation.id;
    }

    // ä¿å­˜æ¶ˆæ¯
    await prisma.aIMessage.createMany({
      data: [
        {
          conversationId,
          role: "USER",
          content: question,
        },
        {
          conversationId,
          role: "ASSISTANT",
          content: response.answer,
          metadata: { sources: response.sources },
          tokenCount: response.tokensUsed,
        },
      ],
    });

    return NextResponse.json({
      answer: response.answer,
      sources: response.sources,
      tokensUsed: response.tokensUsed,
      conversationId,
    });
  } catch (error) {
    console.error("RAG æŸ¥è¯¢é”™è¯¯:", error);
    return NextResponse.json({ error: "æŸ¥è¯¢å¤±è´¥" }, { status: 500 });
  }
}
```

### 4. ç´¢å¼•ç®¡ç† API

åˆ›å»º `src/app/api/ai/rag/index/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { indexPost } from "@/lib/vector/indexer";

export async function POST(request: NextRequest) {
  try {
    const session = await getServerSession(authOptions);
    if (!session?.user || session.user.role !== "ADMIN") {
      return NextResponse.json({ error: "æ— æƒé™" }, { status: 403 });
    }

    const body = await request.json();
    const { postId, force } = body;

    if (postId) {
      // ç´¢å¼•å•ç¯‡æ–‡ç« 
      await indexPost(postId, { force: force || false });
      return NextResponse.json({
        success: true,
        indexed: 1,
        message: "ç´¢å¼•æ„å»ºå®Œæˆ",
      });
    } else {
      // æ‰¹é‡é‡å»ºç´¢å¼•
      // å®ç°æ‰¹é‡ç´¢å¼•é€»è¾‘
      return NextResponse.json({
        success: true,
        message: "æ‰¹é‡ç´¢å¼•åŠŸèƒ½å¼€å‘ä¸­",
      });
    }
  } catch (error) {
    console.error("ç´¢å¼•æ„å»ºé”™è¯¯:", error);
    return NextResponse.json({ error: "ç´¢å¼•æ„å»ºå¤±è´¥" }, { status: 500 });
  }
}
```

åˆ›å»º `src/lib/vector/indexer.ts`:

```typescript
import { prisma } from "../prisma";
import { getAIClient } from "../ai/client";
import { getVectorStore } from "./store";
import { chunkPost } from "./chunker";

export async function indexPost(
  postId: string,
  options: { force?: boolean } = {}
) {
  // 1. è·å–æ–‡ç« 
  const post = await prisma.post.findUnique({
    where: { id: postId },
    include: {
      category: true,
      tags: { include: { tag: true } },
    },
  });

  if (!post) {
    throw new Error("æ–‡ç« ä¸å­˜åœ¨");
  }

  // 2. æ£€æŸ¥æ˜¯å¦å·²ç´¢å¼•
  const existingIndex = await prisma.postVectorIndex.findUnique({
    where: { postId },
  });

  if (existingIndex && !options.force) {
    console.log(`æ–‡ç«  ${postId} å·²ç´¢å¼•,è·³è¿‡`);
    return;
  }

  // 3. åˆ†å—
  const chunks = chunkPost(post.content);

  // 4. ç”Ÿæˆå‘é‡
  const aiClient = getAIClient();
  const texts = chunks.map((chunk) => chunk.content);
  const embeddings = await aiClient.embed(texts);

  // 5. æ„å»ºå‘é‡æ•°æ®
  const vectors = chunks.map((chunk, i) => ({
    embedding: embeddings[i],
    metadata: {
      postId: post.id,
      chunkIndex: chunk.index,
      title: post.title,
      category: post.category?.name,
      tags: post.tags.map((pt) => pt.tag.name),
    },
    document: chunk.content,
  }));

  // 6. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
  const vectorStore = getVectorStore();
  const vectorIds = await vectorStore.upsert(vectors);

  // 7. æ›´æ–°ç´¢å¼•è®°å½•
  await prisma.postVectorIndex.upsert({
    where: { postId },
    create: {
      postId,
      vectorId: vectorIds[0],
      chunkCount: chunks.length,
    },
    update: {
      chunkCount: chunks.length,
      updatedAt: new Date(),
    },
  });

  console.log(`æ–‡ç«  ${postId} ç´¢å¼•å®Œæˆ,å…± ${chunks.length} ä¸ªå—`);
}
```

---

## å‰ç«¯é›†æˆ

### 1. AI åŠ©æ‰‹é¢æ¿ç»„ä»¶

åˆ›å»º `src/components/admin/ai-assistant-panel.tsx`:

```typescript
"use client";

import { useState } from "react";
import { Button } from "@/components/ui/button";
import { Sparkles, Wand2, Loader2 } from "lucide-react";
import { useToast } from "@/hooks/use-toast";

interface AIAssistantPanelProps {
  content: string;
  onInsert: (text: string) => void;
}

export default function AIAssistantPanel({
  content,
  onInsert,
}: AIAssistantPanelProps) {
  const [loading, setLoading] = useState(false);
  const { toast } = useToast();

  const handleGenerateTitle = async () => {
    if (!content.trim()) {
      toast({
        title: "æç¤º",
        description: "è¯·å…ˆè¾“å…¥æ–‡ç« å†…å®¹",
        variant: "default",
      });
      return;
    }

    setLoading(true);
    try {
      const response = await fetch("/api/ai/write/generate", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          type: "title",
          content,
          options: { count: 3 },
        }),
      });

      if (!response.ok) throw new Error("ç”Ÿæˆå¤±è´¥");

      const data = await response.json();
      toast({
        title: "æ ‡é¢˜ç”ŸæˆæˆåŠŸ",
        description: data.results.join(", "),
      });
    } catch (error) {
      toast({
        title: "ç”Ÿæˆå¤±è´¥",
        description: error instanceof Error ? error.message : "æœªçŸ¥é”™è¯¯",
        variant: "destructive",
      });
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="flex gap-2 p-2 border-t">
      <Button
        size="sm"
        variant="outline"
        onClick={handleGenerateTitle}
        disabled={loading}
      >
        {loading ? (
          <Loader2 className="h-4 w-4 mr-2 animate-spin" />
        ) : (
          <Sparkles className="h-4 w-4 mr-2" />
        )}
        ç”Ÿæˆæ ‡é¢˜
      </Button>
      <Button size="sm" variant="outline" disabled>
        <Wand2 className="h-4 w-4 mr-2" />
        ä¼˜åŒ–å†…å®¹
      </Button>
    </div>
  );
}
```

### 2. åœ¨ç¼–è¾‘å™¨ä¸­é›†æˆ

ä¿®æ”¹ `src/components/admin/fullscreen-editor.tsx`,æ·»åŠ  AI åŠ©æ‰‹:

```typescript
// åœ¨ç»„ä»¶é¡¶éƒ¨å¯¼å…¥
import AIAssistantPanel from "./ai-assistant-panel";

// åœ¨ç¼–è¾‘å™¨åº•éƒ¨æ·»åŠ 
<div className="border-t">
  <AIAssistantPanel
    content={content}
    onInsert={(text) => {
      // åœ¨å…‰æ ‡ä½ç½®æ’å…¥æ–‡æœ¬
      const newContent = content + "\n\n" + text;
      onContentChange(newContent);
    }}
  />
</div>
```

---

## æµ‹è¯•å’Œè°ƒè¯•

### 1. æµ‹è¯• AI å®¢æˆ·ç«¯ (Kimi + Ollama)

åˆ›å»º `scripts/test-ai.ts`:

```typescript
import { getAIClient } from "../src/lib/ai/client";

async function test() {
  const client = getAIClient(); // é»˜è®¤ä½¿ç”¨ Kimi

  // æµ‹è¯• Kimi èŠå¤©
  const response = await client.chat([
    { role: "system", content: "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åšå®¢åŠ©æ‰‹" },
    { role: "user", content: "ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±" },
  ]);

  console.log("Kimi å›å¤:", response.content);
  console.log("Token ä½¿ç”¨:", response.tokensUsed);

  // æµ‹è¯• Ollama embedding
  const embeddings = await client.embed("æµ‹è¯•æ–‡æœ¬");
  console.log("Embedding ç»´åº¦:", embeddings[0].length); // åº”è¯¥æ˜¯ 768
}

test().catch(console.error);
```

è¿è¡Œå‰ç¡®ä¿ Ollama æœåŠ¡å·²å¯åŠ¨:

```bash
# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œæµ‹è¯•
tsx scripts/test-ai.ts
```

### 2. æµ‹è¯•å‘é‡å­˜å‚¨

åˆ›å»º `scripts/test-vector.ts`:

```typescript
import { getVectorStore } from "../src/lib/vector/store";
import { getAIClient } from "../src/lib/ai/client";

async function test() {
  const vectorStore = getVectorStore();
  const aiClient = getAIClient();

  // ç”Ÿæˆæµ‹è¯•å‘é‡
  const [embedding] = await aiClient.embed("è¿™æ˜¯ä¸€ç¯‡å…³äº Next.js çš„æ–‡ç« ");

  // å­˜å‚¨
  const ids = await vectorStore.upsert([
    {
      embedding,
      metadata: { postId: "test-1", title: "æµ‹è¯•æ–‡ç« " },
      document: "è¿™æ˜¯ä¸€ç¯‡å…³äº Next.js çš„æ–‡ç« ",
    },
  ]);

  console.log("å­˜å‚¨æˆåŠŸ,ID:", ids[0]);

  // æœç´¢
  const [queryVector] = await aiClient.embed("Next.js æ¡†æ¶");
  const results = await vectorStore.search(queryVector, { limit: 5 });

  console.log("æœç´¢ç»“æœ:", results);
}

test().catch(console.error);
```

### 3. æµ‹è¯• RAG

åˆ›å»º `scripts/test-rag.ts`:

```typescript
import { ragQuery } from "../src/lib/ai/rag";

async function test() {
  const response = await ragQuery("Next.js æ˜¯ä»€ä¹ˆ?", {
    limit: 3,
  });

  console.log("å›ç­”:", response.answer);
  console.log("æ¥æº:", response.sources);
}

test().catch(console.error);
```

---

## ä¸‹ä¸€æ­¥

1. âœ… å®ŒæˆåŸºç¡€ AI åŠŸèƒ½é›†æˆ
2. âœ… å®ç° RAG çŸ¥è¯†åº“
3. â³ å®Œå–„å‰ç«¯ UI
4. â³ æ·»åŠ æ›´å¤š Prompt æ¨¡æ¿
5. â³ æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜
6. â³ æ·»åŠ ä½¿ç”¨ç»Ÿè®¡å’Œç›‘æ§

---

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•åˆ‡æ¢ AI æ¨¡å‹?

A: ä¿®æ”¹ `.env.local` ä¸­çš„ `AI_PROVIDER`:

- `kimi`: ä½¿ç”¨ Moonshot AI (æ¨èï¼Œä¸­æ–‡æ•ˆæœå¥½ï¼Œæœ‰å…è´¹é¢åº¦)
- `openai`: ä½¿ç”¨ OpenAI GPT (æ”¶è´¹)
- `ollama`: ä½¿ç”¨æœ¬åœ°æ¨¡å‹ (å…è´¹)

### Q: å¦‚ä½•è·å– Kimi API Key?

A: è®¿é—® [Moonshot AI å¼€æ”¾å¹³å°](https://platform.moonshot.cn/) æ³¨å†Œå¹¶åˆ›å»º API Keyã€‚æ–°ç”¨æˆ·æœ‰å…è´¹é¢åº¦ã€‚

### Q: å¦‚ä½•å¯åŠ¨ Ollama embedding æœåŠ¡?

A:

```bash
# 1. ç¡®ä¿ Ollama å·²å®‰è£…
ollama --version

# 2. æ‹‰å– embedding æ¨¡å‹ (é¦–æ¬¡éœ€è¦)
ollama pull nomic-embed-text

# 3. å¯åŠ¨ Ollama æœåŠ¡ (å¦‚æœæ²¡æœ‰è‡ªåŠ¨å¯åŠ¨)
ollama serve
```

### Q: å‘é‡æ•°æ®åº“æ•°æ®å­˜å‚¨åœ¨å“ªé‡Œ?

A: Chroma æ•°æ®å­˜å‚¨åœ¨ `./data/chroma` ç›®å½•ï¼Œå®Œå…¨æœ¬åœ°åŒ–ï¼Œæ— éœ€ç½‘ç»œã€‚

### Q: å¦‚ä½•æ‰¹é‡ç´¢å¼•å·²æœ‰æ–‡ç« ?

A: åˆ›å»ºè„šæœ¬éå†æ‰€æœ‰æ–‡ç« å¹¶è°ƒç”¨ `indexPost` å‡½æ•°ã€‚

### Q: æˆæœ¬å¦‚ä½•æ§åˆ¶?

A: æœ¬æ–¹æ¡ˆ**å®Œå…¨å…è´¹**:

- **Kimi**: æœ‰å…è´¹é¢åº¦ï¼Œä¸ªäººåšå®¢åŸºæœ¬å¤Ÿç”¨
- **Chroma**: å¼€æºå…è´¹
- **Ollama embedding**: æœ¬åœ°è¿è¡Œï¼Œå®Œå…¨å…è´¹

---

## å‚è€ƒèµ„æº

- [Kimi API æ–‡æ¡£ (Moonshot AI)](https://platform.moonshot.cn/docs)
- [Chroma æ–‡æ¡£](https://docs.trychroma.com/)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs)
- [RAG æœ€ä½³å®è·µ](https://www.pinecone.io/learn/retrieval-augmented-generation/)
